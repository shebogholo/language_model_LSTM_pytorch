{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "import math \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "embedding_dim = 200\n",
    "hidden_size = 200\n",
    "sequence_length = 35\n",
    "learning_rate = 20\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "display_interval = 100\n",
    "epochs = 2\n",
    "clip_gradient = 0.25\n",
    "tie_weights = False\n",
    "eval_batch_size = 10\n",
    "save = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word) \n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            return self.word2idx[word]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as file:\n",
    "            tokens = 0\n",
    "            for line in file:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as file:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in file:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('wikitext/')\n",
    "num_tokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack data one after another \n",
    "def create_batch(data, batch_size):\n",
    "    num_batch = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, num_batch * batch_size)\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate trainig batch\n",
    "train_data = create_batch(corpus.train, batch_size)\n",
    "valid_data = create_batch(corpus.valid, batch_size)\n",
    "test_data =  create_batch(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   284, 15178,  ...,  1352,  1335,    16],\n",
       "        [    1,   357,    43,  ...,    46,    43,  2015],\n",
       "        [    2,  1496,  7369,  ...,   380,    27, 33001],\n",
       "        ...,\n",
       "        [  357,   415,   173,  ...,   212,    78,  1575],\n",
       "        [ 2520,     9,  3890,  ...,   208,    27,   808],\n",
       "        [   33,    35,    19,  ...,  8832,  6091,   209]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104431, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   652,     9,  ...,    17,    15,   641],\n",
       "        [    1,    17, 10058,  ...,  2058,     0,   127],\n",
       "        [32966,  1874,    26,  ...,    27,     0,    15],\n",
       "        ...,\n",
       "        [ 1450,  5990,     8,  ...,  2228,    37,     1],\n",
       "        [ 3158,     9,     0,  ...,  8587,     9,  8627],\n",
       "        [   15,     9, 16497,  ...,  4382,   128,  4249]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10882, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12278, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def get_batch(source, index):\n",
    "    seq_len = min(sequence_length, len(source) - 1 - index)\n",
    "    data = source[index: index+seq_len]\n",
    "    target = source[index+1: index+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = get_batch(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, dropout=0.5, tie_weights=False):\n",
    "        # embedding_dim == input_size\n",
    "        # num_embeddings == vocabulary_size\n",
    "        # hidden_size == number of features in the hidden layer\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, num_embeddings)\n",
    "\n",
    "        # suggested on paper\n",
    "        if tie_weights:\n",
    "            if hidden_size != embedding_dim:\n",
    "                raise ValueError('When using tie weights hidden_size == embedding_size')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.dropout(self.encoder(input))\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.num_layers, batch_size, self.hidden_size), weight.new_zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (encoder): Embedding(33278, 200)\n",
       "  (lstm): LSTM(200, 200, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model\n",
    "model = RNNModel(num_tokens, embedding_dim, hidden_size, num_layers, dropout, tie_weights=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(hidden):\n",
    "    if isinstance(hidden, torch.Tensor):\n",
    "        return hidden.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = repackage_hidden(hidden)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index in range(0, data_source.size(0) - 1, sequence_length):\n",
    "            data, targets = get_batch(data_source, index)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data, targets = data.cuda(), targets.cuda()\n",
    "                \n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, num_tokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "def train():\n",
    "    model.train()\n",
    "    print('*'*102)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for batch, index in enumerate(range(0, train_data.size(0) - 1, sequence_length)):\n",
    "        data, targets = get_batch(train_data, index)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data, targets = data.cuda(), targets.cuda()\n",
    "            \n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        loss = criterion(output.view(-1, num_tokens), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping to avoid exploding gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_gradient)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % display_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / display_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | train loss {:5.2f} | perplexity {:8.2f}'.format(\n",
    "                epochs - 1, \n",
    "                batch, \n",
    "                len(train_data) // sequence_length, \n",
    "                learning_rate,\n",
    "                elapsed * 1000 / display_interval, \n",
    "                cur_loss, \n",
    "                math.exp(cur_loss)))\n",
    "            \n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************\n",
      "| Epoch   1 |   100/ 2983 batches | lr 20.00 | ms/batch 51.40 | train loss  8.01 | perplexity  3004.75\n",
      "| Epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 47.25 | train loss  7.23 | perplexity  1385.10\n",
      "| Epoch   1 |   300/ 2983 batches | lr 20.00 | ms/batch 47.31 | train loss  6.94 | perplexity  1030.64\n",
      "| Epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 47.14 | train loss  6.74 | perplexity   841.99\n",
      "| Epoch   1 |   500/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  6.55 | perplexity   702.43\n",
      "| Epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 47.16 | train loss  6.42 | perplexity   613.00\n",
      "| Epoch   1 |   700/ 2983 batches | lr 20.00 | ms/batch 47.26 | train loss  6.34 | perplexity   566.36\n",
      "| Epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 47.29 | train loss  6.24 | perplexity   514.96\n",
      "| Epoch   1 |   900/ 2983 batches | lr 20.00 | ms/batch 47.41 | train loss  6.15 | perplexity   467.31\n",
      "| Epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.32 | train loss  6.13 | perplexity   459.74\n",
      "| Epoch   1 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.56 | train loss  6.07 | perplexity   433.66\n",
      "| Epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.53 | train loss  6.05 | perplexity   422.34\n",
      "| Epoch   1 |  1300/ 2983 batches | lr 20.00 | ms/batch 48.75 | train loss  5.99 | perplexity   398.69\n",
      "| Epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.41 | train loss  5.88 | perplexity   359.51\n",
      "| Epoch   1 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.29 | train loss  5.95 | perplexity   384.83\n",
      "| Epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.32 | train loss  5.93 | perplexity   375.43\n",
      "| Epoch   1 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.43 | train loss  5.80 | perplexity   331.78\n",
      "| Epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.44 | train loss  5.77 | perplexity   320.40\n",
      "| Epoch   1 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.53 | train loss  5.78 | perplexity   324.41\n",
      "| Epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.45 | train loss  5.72 | perplexity   304.93\n",
      "| Epoch   1 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.73 | train loss  5.68 | perplexity   293.21\n",
      "| Epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.54 | train loss  5.58 | perplexity   265.18\n",
      "| Epoch   1 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.43 | train loss  5.62 | perplexity   274.73\n",
      "| Epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.41 | train loss  5.69 | perplexity   296.56\n",
      "| Epoch   1 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.43 | train loss  5.61 | perplexity   273.53\n",
      "| Epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 48.41 | train loss  5.67 | perplexity   290.68\n",
      "| Epoch   1 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.39 | train loss  5.54 | perplexity   255.30\n",
      "| Epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.42 | train loss  5.51 | perplexity   247.48\n",
      "| Epoch   1 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.38 | train loss  5.51 | perplexity   247.73\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "| End of epoch   1 | time: 146.53s | valid loss  5.58 | valid perplexity   265.23\n",
      "******************************************************************************************************\n",
      "| Epoch   1 |   100/ 2983 batches | lr 20.00 | ms/batch 48.09 | train loss  5.57 | perplexity   262.57\n",
      "| Epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 47.29 | train loss  5.51 | perplexity   246.26\n",
      "| Epoch   1 |   300/ 2983 batches | lr 20.00 | ms/batch 47.44 | train loss  5.51 | perplexity   248.21\n",
      "| Epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 47.28 | train loss  5.53 | perplexity   253.29\n",
      "| Epoch   1 |   500/ 2983 batches | lr 20.00 | ms/batch 47.39 | train loss  5.41 | perplexity   222.70\n",
      "| Epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 47.41 | train loss  5.30 | perplexity   200.23\n",
      "| Epoch   1 |   700/ 2983 batches | lr 20.00 | ms/batch 47.41 | train loss  5.35 | perplexity   211.45\n",
      "| Epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 49.22 | train loss  5.39 | perplexity   218.30\n",
      "| Epoch   1 |   900/ 2983 batches | lr 20.00 | ms/batch 47.69 | train loss  5.35 | perplexity   210.21\n",
      "| Epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 48.25 | train loss  5.34 | perplexity   209.02\n",
      "| Epoch   1 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.58 | train loss  5.30 | perplexity   199.92\n",
      "| Epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.30 | train loss  5.34 | perplexity   209.17\n",
      "| Epoch   1 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.47 | train loss  5.37 | perplexity   214.08\n",
      "| Epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.37 | train loss  5.27 | perplexity   195.13\n",
      "| Epoch   1 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.42 | train loss  5.40 | perplexity   221.42\n",
      "| Epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.68 | train loss  5.37 | perplexity   215.44\n",
      "| Epoch   1 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.52 | train loss  5.25 | perplexity   190.56\n",
      "| Epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.57 | train loss  5.26 | perplexity   193.04\n",
      "| Epoch   1 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.58 | train loss  5.29 | perplexity   198.21\n",
      "| Epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 48.71 | train loss  5.24 | perplexity   189.44\n",
      "| Epoch   1 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.63 | train loss  5.19 | perplexity   180.26\n",
      "| Epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.43 | train loss  5.12 | perplexity   167.34\n",
      "| Epoch   1 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.42 | train loss  5.15 | perplexity   172.37\n",
      "| Epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.56 | train loss  5.26 | perplexity   191.96\n",
      "| Epoch   1 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.48 | train loss  5.16 | perplexity   174.30\n",
      "| Epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.50 | train loss  5.26 | perplexity   192.25\n",
      "| Epoch   1 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.60 | train loss  5.17 | perplexity   175.30\n",
      "| Epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.90 | train loss  5.11 | perplexity   165.94\n",
      "| Epoch   1 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.92 | train loss  5.11 | perplexity   166.15\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "| End of epoch   2 | time: 146.76s | valid loss  5.35 | valid perplexity   209.77\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' *103)\n",
    "        print('| End of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid perplexity {:8.2f}'.format(\n",
    "            epoch, \n",
    "            (time.time() - epoch_start_time),\n",
    "            val_loss, math.exp(val_loss)))\n",
    "        \n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as file:\n",
    "                torch.save(model, file)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Learninig rate annealing\n",
    "            # Cut-off the learning rate by the factor of 4 if no improvement has been seen in validation data.\n",
    "            learning_rate /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| End of evaluation | test loss  5.27 | test perplexity   194.82\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(save, 'rb') as file:\n",
    "    model = torch.load(file)\n",
    "    model.lstm.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('| End of evaluation | test loss {:5.2f} | test perplexity {:8.2f}'.format( test_loss, math.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_file = 'generated.txt'\n",
    "num_words = 500\n",
    "temp = 1.0\n",
    "display_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (encoder): Embedding(33278, 200)\n",
       "  (lstm): LSTM(200, 200, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(save, 'rb') as file:\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.load(file).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "if torch.cuda.is_available():\n",
    "    input = torch.randint(num_tokens, (1, 1), dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/500 words\n",
      "| Generated 100/500 words\n",
      "| Generated 200/500 words\n",
      "| Generated 300/500 words\n",
      "| Generated 400/500 words\n"
     ]
    }
   ],
   "source": [
    "with open(generated_file, 'w') as file:\n",
    "    with torch.no_grad(): \n",
    "        \n",
    "        for i in range(num_words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            \n",
    "            word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            \n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            file.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % display_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
