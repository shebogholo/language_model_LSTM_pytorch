{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "language_model_LSTM_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shebogholo/language_model_LSTM_pytorch/blob/master/language_model_LSTM_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ63VgKucErc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92841950-51e1-402f-9a87-3ee036a5a185"
      },
      "source": [
        "!git clone https://github.com/shebogholo/language_model_LSTM_pytorch.git\n",
        "import os\n",
        "os.chdir('language_model_LSTM_pytorch')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'language_model_LSTM_pytorch' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUSsFHprZsq7",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HnOZFhjZmMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOnrN5DBZ5ZP",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McOsrpl_ZmMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "embedding_dim = 200\n",
        "hidden_size = 200\n",
        "sequence_length = 35\n",
        "learning_rate = 20\n",
        "num_layers = 2\n",
        "dropout = 0.4\n",
        "display_interval = 100\n",
        "epochs = 30\n",
        "clip_gradient = 0.20\n",
        "tie_weights = False\n",
        "eval_batch_size = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrVBEbl2aD5G",
        "colab_type": "text"
      },
      "source": [
        "## Create a dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdG0m6m1ZmMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "    \n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word) \n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            return self.word2idx[word]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8FM3oaIaKwa",
        "colab_type": "text"
      },
      "source": [
        "## Read the corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzGz122vZmMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus(object):\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "    \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as file:\n",
        "            tokens = 0\n",
        "            for line in file:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as file:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in file:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjkaUeq4ZmMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "031f6687-c7ef-464f-9969-9838030f82fa"
      },
      "source": [
        "corpus = Corpus('wikitext')\n",
        "num_tokens = len(corpus.dictionary)\n",
        "num_tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKPpUIanah6P",
        "colab_type": "text"
      },
      "source": [
        "## Create batches of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQQjEoFZmMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stack data one after another \n",
        "def create_batch(data, batch_size):\n",
        "    num_batch = data.size(0) // batch_size\n",
        "    data = data.narrow(0, 0, num_batch * batch_size)\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtR4m_Qj4l9",
        "colab_type": "text"
      },
      "source": [
        "## Generate batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdNVh8mvZmMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = create_batch(corpus.train, batch_size)\n",
        "valid_data = create_batch(corpus.valid, batch_size)\n",
        "test_data =  create_batch(corpus.test, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOvFoI3aZmMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1aa0e4b9-e225-421b-e10c-0a8ccd0d0650"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([104431, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B862c--hZmM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9cd5067c-dc9b-494a-e4ac-3cc8046f423d"
      },
      "source": [
        "valid_data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10882, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp2yWMGWZmNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0ef8c36-4598-4805-e1fc-25791b6e3658"
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12278, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0brlcVyhavBb",
        "colab_type": "text"
      },
      "source": [
        "## Create data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dah3N0TZmNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(source, index):\n",
        "    seq_len = min(sequence_length, len(source) - 1 - index)\n",
        "    data = source[index: index+seq_len]\n",
        "    target = source[index+1: index+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUvZo4axZmNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a770dc7f-76d5-48dd-920f-6e6539f68bfe"
      },
      "source": [
        "data, target = get_batch(train_data, 1)\n",
        "data.shape, target.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([35, 20]), torch.Size([700]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZuh2zAa1mP",
        "colab_type": "text"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugi9MKAOZmOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, dropout=0.5, tie_weights=False):\n",
        "        # embedding_dim == input_size\n",
        "        # num_embeddings == vocabulary_size\n",
        "        # hidden_size == number of features in the hidden layer\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(hidden_size, num_embeddings)\n",
        "\n",
        "        # suggested on paper\n",
        "        if tie_weights:\n",
        "            if hidden_size != embedding_dim:\n",
        "                raise ValueError('When using tie weights hidden_size == embedding_size')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embed = self.dropout(self.encoder(input))\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.num_layers, batch_size, self.hidden_size), weight.new_zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "model = RNNModel(num_tokens, embedding_dim, hidden_size, num_layers, dropout, tie_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOMv-XdxfH5c",
        "colab_type": "text"
      },
      "source": [
        "## Check the available device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGU5rYpkZmOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjxn3sy1goaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b4dbc45b-80ff-4e5a-da46-d73e2d892f75"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (dropout): Dropout(p=0.4)\n",
              "  (encoder): Embedding(33278, 200)\n",
              "  (lstm): LSTM(200, 200, num_layers=2, dropout=0.4)\n",
              "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8XKyiMSe3Yc",
        "colab_type": "text"
      },
      "source": [
        "## Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx9OumAwZmOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2mNE-UffD-O",
        "colab_type": "text"
      },
      "source": [
        "## Innitialize hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrNuhvNEZmOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(hidden):\n",
        "    if isinstance(hidden, torch.Tensor):\n",
        "        return hidden.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in hidden)\n",
        "\n",
        "hidden = model.init_hidden(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1wgVy7IZmOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden = repackage_hidden(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgemEhFsf0Te",
        "colab_type": "text"
      },
      "source": [
        "## Function to evaluate a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdQDy-0nZmO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model evaluation\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for index in range(0, data_source.size(0) - 1, sequence_length):\n",
        "            data, targets = get_batch(data_source, index)\n",
        "            data, targets = data.to(device), targets.to(device)                \n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, num_tokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / len(data_source)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaKwoeCig-i8",
        "colab_type": "text"
      },
      "source": [
        "## Function to train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obiUVudpZmPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model training\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    print('*'*102)\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, index in enumerate(range(0, train_data.size(0) - 1, sequence_length)):\n",
        "        data, targets = get_batch(train_data, index)\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, num_tokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping to avoid exploding gradient\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_gradient)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % display_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / display_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | train loss {:5.2f} | perplexity {:8.2f}'.format(\n",
        "                epoch, \n",
        "                batch, \n",
        "                len(train_data) // sequence_length, \n",
        "                learning_rate,\n",
        "                elapsed * 1000 / display_interval, \n",
        "                cur_loss, \n",
        "                math.exp(cur_loss)))\n",
        "            \n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgmjoaBkhbIt",
        "colab_type": "text"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RWPvxlqZmPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b445d7d-9d58-41d6-fbc1-890ccbd34839"
      },
      "source": [
        "best_val_loss = None\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(epoch)\n",
        "        val_loss = evaluate(valid_data)\n",
        "        print('-' *103)\n",
        "        print('| End of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid perplexity {:8.2f}'.format(\n",
        "            epoch, \n",
        "            (time.time() - epoch_start_time),\n",
        "            val_loss, math.exp(val_loss)))\n",
        "        \n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as file:\n",
        "                torch.save(model, file)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Learninig rate annealing\n",
        "            # Cut-off the learning rate by the factor of 4 if no improvement has been seen in validation data.\n",
        "            learning_rate /= 4.0"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************************************************************************************\n",
            "| Epoch   2 |   100/ 2983 batches | lr 20.00 | ms/batch 48.26 | train loss  8.08 | perplexity  3229.22\n",
            "| Epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 45.82 | train loss  7.30 | perplexity  1478.00\n",
            "| Epoch   2 |   300/ 2983 batches | lr 20.00 | ms/batch 46.27 | train loss  6.99 | perplexity  1086.50\n",
            "| Epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 46.01 | train loss  6.76 | perplexity   858.76\n",
            "| Epoch   2 |   500/ 2983 batches | lr 20.00 | ms/batch 46.17 | train loss  6.58 | perplexity   720.26\n",
            "| Epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 46.26 | train loss  6.44 | perplexity   628.14\n",
            "| Epoch   2 |   700/ 2983 batches | lr 20.00 | ms/batch 46.25 | train loss  6.35 | perplexity   573.30\n",
            "| Epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 46.48 | train loss  6.24 | perplexity   514.72\n",
            "| Epoch   2 |   900/ 2983 batches | lr 20.00 | ms/batch 46.63 | train loss  6.14 | perplexity   464.90\n",
            "| Epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 46.48 | train loss  6.12 | perplexity   454.65\n",
            "| Epoch   2 |  1100/ 2983 batches | lr 20.00 | ms/batch 46.63 | train loss  6.06 | perplexity   429.44\n",
            "| Epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 46.77 | train loss  6.02 | perplexity   411.42\n",
            "| Epoch   2 |  1300/ 2983 batches | lr 20.00 | ms/batch 46.89 | train loss  5.97 | perplexity   392.79\n",
            "| Epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 46.81 | train loss  5.87 | perplexity   353.40\n",
            "| Epoch   2 |  1500/ 2983 batches | lr 20.00 | ms/batch 46.93 | train loss  5.95 | perplexity   382.57\n",
            "| Epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 46.94 | train loss  5.92 | perplexity   373.65\n",
            "| Epoch   2 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  5.79 | perplexity   328.14\n",
            "| Epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 46.91 | train loss  5.76 | perplexity   317.26\n",
            "| Epoch   2 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  5.77 | perplexity   320.77\n",
            "| Epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 46.91 | train loss  5.72 | perplexity   303.95\n",
            "| Epoch   2 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.69 | perplexity   294.83\n",
            "| Epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.58 | perplexity   264.99\n",
            "| Epoch   2 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  5.61 | perplexity   273.94\n",
            "| Epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 46.46 | train loss  5.70 | perplexity   298.80\n",
            "| Epoch   2 |  2500/ 2983 batches | lr 20.00 | ms/batch 46.87 | train loss  5.61 | perplexity   273.95\n",
            "| Epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 46.86 | train loss  5.66 | perplexity   287.92\n",
            "| Epoch   2 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.54 | perplexity   255.18\n",
            "| Epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 46.86 | train loss  5.50 | perplexity   245.24\n",
            "| Epoch   2 |  2900/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  5.49 | perplexity   243.31\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   1 | time: 143.72s | valid loss  5.55 | valid perplexity   258.35\n",
            "******************************************************************************************************\n",
            "| Epoch   3 |   100/ 2983 batches | lr 20.00 | ms/batch 47.51 | train loss  5.55 | perplexity   256.76\n",
            "| Epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  5.49 | perplexity   242.11\n",
            "| Epoch   3 |   300/ 2983 batches | lr 20.00 | ms/batch 46.97 | train loss  5.50 | perplexity   245.63\n",
            "| Epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  5.52 | perplexity   249.23\n",
            "| Epoch   3 |   500/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.40 | perplexity   220.41\n",
            "| Epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  5.29 | perplexity   199.03\n",
            "| Epoch   3 |   700/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  5.34 | perplexity   207.57\n",
            "| Epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 46.94 | train loss  5.37 | perplexity   214.15\n",
            "| Epoch   3 |   900/ 2983 batches | lr 20.00 | ms/batch 46.96 | train loss  5.33 | perplexity   207.14\n",
            "| Epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 46.97 | train loss  5.33 | perplexity   205.54\n",
            "| Epoch   3 |  1100/ 2983 batches | lr 20.00 | ms/batch 46.92 | train loss  5.30 | perplexity   199.42\n",
            "| Epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  5.33 | perplexity   205.66\n",
            "| Epoch   3 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  5.35 | perplexity   210.48\n",
            "| Epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  5.25 | perplexity   190.71\n",
            "| Epoch   3 |  1500/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  5.37 | perplexity   215.31\n",
            "| Epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.34 | perplexity   208.63\n",
            "| Epoch   3 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  5.23 | perplexity   186.05\n",
            "| Epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.16 | train loss  5.22 | perplexity   185.62\n",
            "| Epoch   3 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  5.25 | perplexity   190.45\n",
            "| Epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 46.94 | train loss  5.21 | perplexity   183.19\n",
            "| Epoch   3 |  2100/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  5.16 | perplexity   174.16\n",
            "| Epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  5.10 | perplexity   164.00\n",
            "| Epoch   3 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  5.12 | perplexity   166.80\n",
            "| Epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  5.23 | perplexity   187.37\n",
            "| Epoch   3 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  5.14 | perplexity   170.34\n",
            "| Epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  5.23 | perplexity   185.91\n",
            "| Epoch   3 |  2700/ 2983 batches | lr 20.00 | ms/batch 46.88 | train loss  5.13 | perplexity   168.43\n",
            "| Epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  5.08 | perplexity   160.20\n",
            "| Epoch   3 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.02 | train loss  5.08 | perplexity   160.70\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   2 | time: 144.54s | valid loss  5.32 | valid perplexity   204.49\n",
            "******************************************************************************************************\n",
            "| Epoch   4 |   100/ 2983 batches | lr 20.00 | ms/batch 47.48 | train loss  5.17 | perplexity   176.08\n",
            "| Epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 46.96 | train loss  5.12 | perplexity   166.90\n",
            "| Epoch   4 |   300/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  5.14 | perplexity   171.54\n",
            "| Epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 46.84 | train loss  5.17 | perplexity   176.72\n",
            "| Epoch   4 |   500/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  5.03 | perplexity   153.23\n",
            "| Epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.93 | perplexity   138.46\n",
            "| Epoch   4 |   700/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  5.00 | perplexity   147.68\n",
            "| Epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 47.02 | train loss  5.06 | perplexity   157.17\n",
            "| Epoch   4 |   900/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  5.03 | perplexity   152.92\n",
            "| Epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  5.01 | perplexity   150.16\n",
            "| Epoch   4 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.12 | train loss  4.99 | perplexity   146.30\n",
            "| Epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  5.04 | perplexity   154.27\n",
            "| Epoch   4 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  5.08 | perplexity   160.56\n",
            "| Epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.98 | perplexity   145.86\n",
            "| Epoch   4 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  5.11 | perplexity   165.22\n",
            "| Epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  5.10 | perplexity   163.52\n",
            "| Epoch   4 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.97 | perplexity   144.00\n",
            "| Epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.98 | perplexity   145.82\n",
            "| Epoch   4 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  5.01 | perplexity   149.38\n",
            "| Epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.97 | perplexity   144.19\n",
            "| Epoch   4 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.91 | perplexity   135.47\n",
            "| Epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  4.86 | perplexity   129.34\n",
            "| Epoch   4 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.88 | perplexity   132.25\n",
            "| Epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  5.02 | perplexity   150.69\n",
            "| Epoch   4 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.90 | perplexity   134.66\n",
            "| Epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  5.00 | perplexity   148.66\n",
            "| Epoch   4 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.92 | perplexity   136.75\n",
            "| Epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.85 | perplexity   127.83\n",
            "| Epoch   4 |  2900/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  4.87 | perplexity   130.30\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   3 | time: 144.67s | valid loss  5.24 | valid perplexity   187.80\n",
            "******************************************************************************************************\n",
            "| Epoch   5 |   100/ 2983 batches | lr 20.00 | ms/batch 47.53 | train loss  4.96 | perplexity   143.27\n",
            "| Epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.92 | perplexity   137.13\n",
            "| Epoch   5 |   300/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.95 | perplexity   140.61\n",
            "| Epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 46.90 | train loss  4.98 | perplexity   145.75\n",
            "| Epoch   5 |   500/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.83 | perplexity   124.94\n",
            "| Epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.73 | perplexity   113.58\n",
            "| Epoch   5 |   700/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  4.80 | perplexity   121.09\n",
            "| Epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.87 | perplexity   130.27\n",
            "| Epoch   5 |   900/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.85 | perplexity   128.37\n",
            "| Epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.83 | perplexity   125.59\n",
            "| Epoch   5 |  1100/ 2983 batches | lr 20.00 | ms/batch 46.83 | train loss  4.81 | perplexity   122.25\n",
            "| Epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.87 | perplexity   130.01\n",
            "| Epoch   5 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.91 | perplexity   136.25\n",
            "| Epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  4.81 | perplexity   122.71\n",
            "| Epoch   5 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.94 | perplexity   139.88\n",
            "| Epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.21 | train loss  4.93 | perplexity   138.56\n",
            "| Epoch   5 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.81 | perplexity   122.42\n",
            "| Epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 46.93 | train loss  4.83 | perplexity   124.99\n",
            "| Epoch   5 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.86 | perplexity   128.90\n",
            "| Epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.82 | perplexity   124.17\n",
            "| Epoch   5 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.76 | perplexity   116.91\n",
            "| Epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.72 | perplexity   112.00\n",
            "| Epoch   5 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.73 | perplexity   112.90\n",
            "| Epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 46.93 | train loss  4.86 | perplexity   129.32\n",
            "| Epoch   5 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.75 | perplexity   115.60\n",
            "| Epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  4.86 | perplexity   128.64\n",
            "| Epoch   5 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.78 | perplexity   118.81\n",
            "| Epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.19 | train loss  4.71 | perplexity   111.14\n",
            "| Epoch   5 |  2900/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  4.73 | perplexity   113.52\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   4 | time: 144.68s | valid loss  5.17 | valid perplexity   176.35\n",
            "******************************************************************************************************\n",
            "| Epoch   6 |   100/ 2983 batches | lr 20.00 | ms/batch 47.44 | train loss  4.82 | perplexity   124.55\n",
            "| Epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.79 | perplexity   120.17\n",
            "| Epoch   6 |   300/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  4.81 | perplexity   123.01\n",
            "| Epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 46.92 | train loss  4.84 | perplexity   126.66\n",
            "| Epoch   6 |   500/ 2983 batches | lr 20.00 | ms/batch 46.96 | train loss  4.69 | perplexity   109.02\n",
            "| Epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 46.89 | train loss  4.61 | perplexity    99.99\n",
            "| Epoch   6 |   700/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  4.66 | perplexity   106.13\n",
            "| Epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.74 | perplexity   114.37\n",
            "| Epoch   6 |   900/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  4.73 | perplexity   112.92\n",
            "| Epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.71 | perplexity   110.81\n",
            "| Epoch   6 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.69 | perplexity   109.02\n",
            "| Epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.75 | perplexity   115.56\n",
            "| Epoch   6 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.80 | perplexity   121.40\n",
            "| Epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.70 | perplexity   110.14\n",
            "| Epoch   6 |  1500/ 2983 batches | lr 20.00 | ms/batch 46.98 | train loss  4.82 | perplexity   124.58\n",
            "| Epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.83 | perplexity   124.71\n",
            "| Epoch   6 |  1700/ 2983 batches | lr 20.00 | ms/batch 46.87 | train loss  4.71 | perplexity   110.99\n",
            "| Epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.72 | perplexity   112.45\n",
            "| Epoch   6 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  4.74 | perplexity   114.96\n",
            "| Epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.02 | train loss  4.71 | perplexity   111.44\n",
            "| Epoch   6 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.65 | perplexity   104.24\n",
            "| Epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.61 | perplexity   100.10\n",
            "| Epoch   6 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  4.62 | perplexity   101.10\n",
            "| Epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.75 | perplexity   116.15\n",
            "| Epoch   6 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.65 | perplexity   105.08\n",
            "| Epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 46.93 | train loss  4.75 | perplexity   115.61\n",
            "| Epoch   6 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.14 | train loss  4.68 | perplexity   108.11\n",
            "| Epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.61 | perplexity   100.79\n",
            "| Epoch   6 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.64 | perplexity   103.08\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   5 | time: 144.56s | valid loss  5.11 | valid perplexity   166.48\n",
            "******************************************************************************************************\n",
            "| Epoch   7 |   100/ 2983 batches | lr 20.00 | ms/batch 47.45 | train loss  4.73 | perplexity   113.24\n",
            "| Epoch   7 |   200/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.68 | perplexity   108.14\n",
            "| Epoch   7 |   300/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.71 | perplexity   110.53\n",
            "| Epoch   7 |   400/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.74 | perplexity   114.67\n",
            "| Epoch   7 |   500/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.59 | perplexity    98.65\n",
            "| Epoch   7 |   600/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.50 | perplexity    90.25\n",
            "| Epoch   7 |   700/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.57 | perplexity    96.35\n",
            "| Epoch   7 |   800/ 2983 batches | lr 20.00 | ms/batch 47.01 | train loss  4.65 | perplexity   104.47\n",
            "| Epoch   7 |   900/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  4.64 | perplexity   103.33\n",
            "| Epoch   7 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.62 | perplexity   101.53\n",
            "| Epoch   7 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.02 | train loss  4.60 | perplexity    99.65\n",
            "| Epoch   7 |  1200/ 2983 batches | lr 20.00 | ms/batch 46.81 | train loss  4.66 | perplexity   105.47\n",
            "| Epoch   7 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.71 | perplexity   111.31\n",
            "| Epoch   7 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.12 | train loss  4.61 | perplexity   100.80\n",
            "| Epoch   7 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.73 | perplexity   113.45\n",
            "| Epoch   7 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.75 | perplexity   115.33\n",
            "| Epoch   7 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.23 | train loss  4.62 | perplexity   101.05\n",
            "| Epoch   7 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.64 | perplexity   103.09\n",
            "| Epoch   7 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.66 | perplexity   106.15\n",
            "| Epoch   7 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.62 | perplexity   101.63\n",
            "| Epoch   7 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.57 | perplexity    96.43\n",
            "| Epoch   7 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.52 | perplexity    91.92\n",
            "| Epoch   7 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.53 | perplexity    93.00\n",
            "| Epoch   7 |  2400/ 2983 batches | lr 20.00 | ms/batch 46.96 | train loss  4.67 | perplexity   107.16\n",
            "| Epoch   7 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.56 | perplexity    96.00\n",
            "| Epoch   7 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.67 | perplexity   107.22\n",
            "| Epoch   7 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.60 | perplexity    99.70\n",
            "| Epoch   7 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.54 | perplexity    93.58\n",
            "| Epoch   7 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.56 | perplexity    95.17\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   6 | time: 144.74s | valid loss  5.09 | valid perplexity   163.10\n",
            "******************************************************************************************************\n",
            "| Epoch   8 |   100/ 2983 batches | lr 20.00 | ms/batch 47.64 | train loss  4.65 | perplexity   104.78\n",
            "| Epoch   8 |   200/ 2983 batches | lr 20.00 | ms/batch 46.94 | train loss  4.61 | perplexity   100.31\n",
            "| Epoch   8 |   300/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.63 | perplexity   102.53\n",
            "| Epoch   8 |   400/ 2983 batches | lr 20.00 | ms/batch 47.21 | train loss  4.67 | perplexity   106.27\n",
            "| Epoch   8 |   500/ 2983 batches | lr 20.00 | ms/batch 47.26 | train loss  4.51 | perplexity    90.85\n",
            "| Epoch   8 |   600/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.43 | perplexity    84.09\n",
            "| Epoch   8 |   700/ 2983 batches | lr 20.00 | ms/batch 47.19 | train loss  4.48 | perplexity    88.55\n",
            "| Epoch   8 |   800/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  4.58 | perplexity    97.81\n",
            "| Epoch   8 |   900/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.57 | perplexity    96.46\n",
            "| Epoch   8 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.27 | train loss  4.55 | perplexity    94.30\n",
            "| Epoch   8 |  1100/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.53 | perplexity    92.41\n",
            "| Epoch   8 |  1200/ 2983 batches | lr 20.00 | ms/batch 47.20 | train loss  4.59 | perplexity    98.23\n",
            "| Epoch   8 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.64 | perplexity   103.56\n",
            "| Epoch   8 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.55 | perplexity    94.56\n",
            "| Epoch   8 |  1500/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  4.67 | perplexity   106.24\n",
            "| Epoch   8 |  1600/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.68 | perplexity   107.73\n",
            "| Epoch   8 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.17 | train loss  4.56 | perplexity    95.13\n",
            "| Epoch   8 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.58 | perplexity    97.26\n",
            "| Epoch   8 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.14 | train loss  4.60 | perplexity    99.58\n",
            "| Epoch   8 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.26 | train loss  4.56 | perplexity    96.03\n",
            "| Epoch   8 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.51 | perplexity    90.49\n",
            "| Epoch   8 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.18 | train loss  4.47 | perplexity    87.16\n",
            "| Epoch   8 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.47 | perplexity    87.20\n",
            "| Epoch   8 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.15 | train loss  4.62 | perplexity   101.32\n",
            "| Epoch   8 |  2500/ 2983 batches | lr 20.00 | ms/batch 47.14 | train loss  4.50 | perplexity    90.37\n",
            "| Epoch   8 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.60 | perplexity    99.82\n",
            "| Epoch   8 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.20 | train loss  4.54 | perplexity    93.43\n",
            "| Epoch   8 |  2800/ 2983 batches | lr 20.00 | ms/batch 47.10 | train loss  4.47 | perplexity    86.97\n",
            "| Epoch   8 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.25 | train loss  4.49 | perplexity    89.03\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   7 | time: 144.91s | valid loss  5.08 | valid perplexity   161.20\n",
            "******************************************************************************************************\n",
            "| Epoch   9 |   100/ 2983 batches | lr 20.00 | ms/batch 47.56 | train loss  4.59 | perplexity    98.73\n",
            "| Epoch   9 |   200/ 2983 batches | lr 20.00 | ms/batch 47.06 | train loss  4.55 | perplexity    94.56\n",
            "| Epoch   9 |   300/ 2983 batches | lr 20.00 | ms/batch 46.92 | train loss  4.57 | perplexity    96.77\n",
            "| Epoch   9 |   400/ 2983 batches | lr 20.00 | ms/batch 47.02 | train loss  4.61 | perplexity   100.03\n",
            "| Epoch   9 |   500/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.46 | perplexity    86.65\n",
            "| Epoch   9 |   600/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.38 | perplexity    80.16\n",
            "| Epoch   9 |   700/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.43 | perplexity    83.94\n",
            "| Epoch   9 |   800/ 2983 batches | lr 20.00 | ms/batch 47.13 | train loss  4.52 | perplexity    92.19\n",
            "| Epoch   9 |   900/ 2983 batches | lr 20.00 | ms/batch 47.03 | train loss  4.51 | perplexity    91.26\n",
            "| Epoch   9 |  1000/ 2983 batches | lr 20.00 | ms/batch 47.00 | train loss  4.49 | perplexity    89.04\n",
            "| Epoch   9 |  1100/ 2983 batches | lr 20.00 | ms/batch 46.91 | train loss  4.47 | perplexity    87.31\n",
            "| Epoch   9 |  1200/ 2983 batches | lr 20.00 | ms/batch 46.91 | train loss  4.52 | perplexity    92.23\n",
            "| Epoch   9 |  1300/ 2983 batches | lr 20.00 | ms/batch 47.19 | train loss  4.58 | perplexity    97.75\n",
            "| Epoch   9 |  1400/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.49 | perplexity    89.29\n",
            "| Epoch   9 |  1500/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.60 | perplexity    99.16\n",
            "| Epoch   9 |  1600/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  4.62 | perplexity   101.91\n",
            "| Epoch   9 |  1700/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.50 | perplexity    89.58\n",
            "| Epoch   9 |  1800/ 2983 batches | lr 20.00 | ms/batch 47.08 | train loss  4.52 | perplexity    91.82\n",
            "| Epoch   9 |  1900/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.54 | perplexity    93.86\n",
            "| Epoch   9 |  2000/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.50 | perplexity    89.94\n",
            "| Epoch   9 |  2100/ 2983 batches | lr 20.00 | ms/batch 47.14 | train loss  4.44 | perplexity    85.00\n",
            "| Epoch   9 |  2200/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.41 | perplexity    82.19\n",
            "| Epoch   9 |  2300/ 2983 batches | lr 20.00 | ms/batch 47.09 | train loss  4.41 | perplexity    82.51\n",
            "| Epoch   9 |  2400/ 2983 batches | lr 20.00 | ms/batch 47.11 | train loss  4.55 | perplexity    94.94\n",
            "| Epoch   9 |  2500/ 2983 batches | lr 20.00 | ms/batch 46.99 | train loss  4.45 | perplexity    85.46\n",
            "| Epoch   9 |  2600/ 2983 batches | lr 20.00 | ms/batch 47.07 | train loss  4.57 | perplexity    96.64\n",
            "| Epoch   9 |  2700/ 2983 batches | lr 20.00 | ms/batch 47.04 | train loss  4.49 | perplexity    89.44\n",
            "| Epoch   9 |  2800/ 2983 batches | lr 20.00 | ms/batch 46.95 | train loss  4.42 | perplexity    82.73\n",
            "| Epoch   9 |  2900/ 2983 batches | lr 20.00 | ms/batch 47.05 | train loss  4.44 | perplexity    84.95\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   8 | time: 144.67s | valid loss  5.09 | valid perplexity   162.85\n",
            "******************************************************************************************************\n",
            "| Epoch  10 |   100/ 2983 batches | lr 5.00 | ms/batch 47.58 | train loss  4.58 | perplexity    97.20\n",
            "| Epoch  10 |   200/ 2983 batches | lr 5.00 | ms/batch 47.04 | train loss  4.55 | perplexity    95.08\n",
            "| Epoch  10 |   300/ 2983 batches | lr 5.00 | ms/batch 47.16 | train loss  4.54 | perplexity    93.86\n",
            "| Epoch  10 |   400/ 2983 batches | lr 5.00 | ms/batch 47.11 | train loss  4.57 | perplexity    96.92\n",
            "| Epoch  10 |   500/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.41 | perplexity    82.20\n",
            "| Epoch  10 |   600/ 2983 batches | lr 5.00 | ms/batch 47.07 | train loss  4.33 | perplexity    76.17\n",
            "| Epoch  10 |   700/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.40 | perplexity    81.72\n",
            "| Epoch  10 |   800/ 2983 batches | lr 5.00 | ms/batch 46.92 | train loss  4.47 | perplexity    86.98\n",
            "| Epoch  10 |   900/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.45 | perplexity    85.79\n",
            "| Epoch  10 |  1000/ 2983 batches | lr 5.00 | ms/batch 47.00 | train loss  4.40 | perplexity    81.60\n",
            "| Epoch  10 |  1100/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.39 | perplexity    80.70\n",
            "| Epoch  10 |  1200/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.44 | perplexity    84.38\n",
            "| Epoch  10 |  1300/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.50 | perplexity    90.21\n",
            "| Epoch  10 |  1400/ 2983 batches | lr 5.00 | ms/batch 47.00 | train loss  4.36 | perplexity    78.52\n",
            "| Epoch  10 |  1500/ 2983 batches | lr 5.00 | ms/batch 47.26 | train loss  4.46 | perplexity    86.18\n",
            "| Epoch  10 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.06 | train loss  4.49 | perplexity    89.52\n",
            "| Epoch  10 |  1700/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.35 | perplexity    77.73\n",
            "| Epoch  10 |  1800/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.38 | perplexity    79.50\n",
            "| Epoch  10 |  1900/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.41 | perplexity    81.95\n",
            "| Epoch  10 |  2000/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.36 | perplexity    78.39\n",
            "| Epoch  10 |  2100/ 2983 batches | lr 5.00 | ms/batch 47.11 | train loss  4.29 | perplexity    73.16\n",
            "| Epoch  10 |  2200/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.22 | perplexity    68.18\n",
            "| Epoch  10 |  2300/ 2983 batches | lr 5.00 | ms/batch 47.11 | train loss  4.22 | perplexity    67.81\n",
            "| Epoch  10 |  2400/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.35 | perplexity    77.60\n",
            "| Epoch  10 |  2500/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.25 | perplexity    70.15\n",
            "| Epoch  10 |  2600/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.37 | perplexity    78.81\n",
            "| Epoch  10 |  2700/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.28 | perplexity    72.13\n",
            "| Epoch  10 |  2800/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.19 | perplexity    65.86\n",
            "| Epoch  10 |  2900/ 2983 batches | lr 5.00 | ms/batch 47.23 | train loss  4.20 | perplexity    66.52\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch   9 | time: 144.84s | valid loss  4.97 | valid perplexity   143.35\n",
            "******************************************************************************************************\n",
            "| Epoch  11 |   100/ 2983 batches | lr 5.00 | ms/batch 47.66 | train loss  4.45 | perplexity    85.35\n",
            "| Epoch  11 |   200/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.42 | perplexity    82.90\n",
            "| Epoch  11 |   300/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.43 | perplexity    83.60\n",
            "| Epoch  11 |   400/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.46 | perplexity    86.62\n",
            "| Epoch  11 |   500/ 2983 batches | lr 5.00 | ms/batch 46.86 | train loss  4.31 | perplexity    74.08\n",
            "| Epoch  11 |   600/ 2983 batches | lr 5.00 | ms/batch 46.90 | train loss  4.23 | perplexity    68.96\n",
            "| Epoch  11 |   700/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.29 | perplexity    73.32\n",
            "| Epoch  11 |   800/ 2983 batches | lr 5.00 | ms/batch 47.20 | train loss  4.37 | perplexity    79.12\n",
            "| Epoch  11 |   900/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.36 | perplexity    77.93\n",
            "| Epoch  11 |  1000/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.32 | perplexity    74.82\n",
            "| Epoch  11 |  1100/ 2983 batches | lr 5.00 | ms/batch 47.21 | train loss  4.31 | perplexity    74.60\n",
            "| Epoch  11 |  1200/ 2983 batches | lr 5.00 | ms/batch 46.89 | train loss  4.36 | perplexity    78.02\n",
            "| Epoch  11 |  1300/ 2983 batches | lr 5.00 | ms/batch 47.22 | train loss  4.43 | perplexity    83.70\n",
            "| Epoch  11 |  1400/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.30 | perplexity    73.42\n",
            "| Epoch  11 |  1500/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.40 | perplexity    81.41\n",
            "| Epoch  11 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.44 | perplexity    84.45\n",
            "| Epoch  11 |  1700/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.30 | perplexity    73.68\n",
            "| Epoch  11 |  1800/ 2983 batches | lr 5.00 | ms/batch 47.19 | train loss  4.32 | perplexity    75.51\n",
            "| Epoch  11 |  1900/ 2983 batches | lr 5.00 | ms/batch 47.22 | train loss  4.36 | perplexity    78.04\n",
            "| Epoch  11 |  2000/ 2983 batches | lr 5.00 | ms/batch 47.00 | train loss  4.32 | perplexity    75.18\n",
            "| Epoch  11 |  2100/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.25 | perplexity    70.13\n",
            "| Epoch  11 |  2200/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.19 | perplexity    65.79\n",
            "| Epoch  11 |  2300/ 2983 batches | lr 5.00 | ms/batch 47.22 | train loss  4.18 | perplexity    65.35\n",
            "| Epoch  11 |  2400/ 2983 batches | lr 5.00 | ms/batch 47.21 | train loss  4.32 | perplexity    75.03\n",
            "| Epoch  11 |  2500/ 2983 batches | lr 5.00 | ms/batch 47.23 | train loss  4.22 | perplexity    68.01\n",
            "| Epoch  11 |  2600/ 2983 batches | lr 5.00 | ms/batch 47.14 | train loss  4.34 | perplexity    76.70\n",
            "| Epoch  11 |  2700/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.25 | perplexity    70.29\n",
            "| Epoch  11 |  2800/ 2983 batches | lr 5.00 | ms/batch 47.22 | train loss  4.17 | perplexity    64.90\n",
            "| Epoch  11 |  2900/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.18 | perplexity    65.66\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  10 | time: 144.95s | valid loss  4.96 | valid perplexity   142.12\n",
            "******************************************************************************************************\n",
            "| Epoch  12 |   100/ 2983 batches | lr 5.00 | ms/batch 47.62 | train loss  4.39 | perplexity    81.03\n",
            "| Epoch  12 |   200/ 2983 batches | lr 5.00 | ms/batch 46.98 | train loss  4.37 | perplexity    79.39\n",
            "| Epoch  12 |   300/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.39 | perplexity    80.44\n",
            "| Epoch  12 |   400/ 2983 batches | lr 5.00 | ms/batch 46.88 | train loss  4.42 | perplexity    82.88\n",
            "| Epoch  12 |   500/ 2983 batches | lr 5.00 | ms/batch 46.34 | train loss  4.26 | perplexity    70.74\n",
            "| Epoch  12 |   600/ 2983 batches | lr 5.00 | ms/batch 47.16 | train loss  4.18 | perplexity    65.54\n",
            "| Epoch  12 |   700/ 2983 batches | lr 5.00 | ms/batch 47.26 | train loss  4.25 | perplexity    69.81\n",
            "| Epoch  12 |   800/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.32 | perplexity    75.42\n",
            "| Epoch  12 |   900/ 2983 batches | lr 5.00 | ms/batch 47.06 | train loss  4.32 | perplexity    75.25\n",
            "| Epoch  12 |  1000/ 2983 batches | lr 5.00 | ms/batch 47.00 | train loss  4.27 | perplexity    71.70\n",
            "| Epoch  12 |  1100/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.26 | perplexity    71.14\n",
            "| Epoch  12 |  1200/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.34 | perplexity    76.48\n",
            "| Epoch  12 |  1300/ 2983 batches | lr 5.00 | ms/batch 47.20 | train loss  4.39 | perplexity    80.82\n",
            "| Epoch  12 |  1400/ 2983 batches | lr 5.00 | ms/batch 47.23 | train loss  4.26 | perplexity    70.65\n",
            "| Epoch  12 |  1500/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.36 | perplexity    78.39\n",
            "| Epoch  12 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.14 | train loss  4.40 | perplexity    81.85\n",
            "| Epoch  12 |  1700/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.27 | perplexity    71.36\n",
            "| Epoch  12 |  1800/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.29 | perplexity    72.99\n",
            "| Epoch  12 |  1900/ 2983 batches | lr 5.00 | ms/batch 47.14 | train loss  4.32 | perplexity    75.54\n",
            "| Epoch  12 |  2000/ 2983 batches | lr 5.00 | ms/batch 47.19 | train loss  4.29 | perplexity    73.07\n",
            "| Epoch  12 |  2100/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.22 | perplexity    67.99\n",
            "| Epoch  12 |  2200/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.16 | perplexity    64.23\n",
            "| Epoch  12 |  2300/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.15 | perplexity    63.48\n",
            "| Epoch  12 |  2400/ 2983 batches | lr 5.00 | ms/batch 47.27 | train loss  4.28 | perplexity    72.47\n",
            "| Epoch  12 |  2500/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.20 | perplexity    66.75\n",
            "| Epoch  12 |  2600/ 2983 batches | lr 5.00 | ms/batch 47.23 | train loss  4.31 | perplexity    74.46\n",
            "| Epoch  12 |  2700/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.23 | perplexity    68.91\n",
            "| Epoch  12 |  2800/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.15 | perplexity    63.66\n",
            "| Epoch  12 |  2900/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.16 | perplexity    64.06\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  11 | time: 144.90s | valid loss  4.95 | valid perplexity   140.82\n",
            "******************************************************************************************************\n",
            "| Epoch  13 |   100/ 2983 batches | lr 5.00 | ms/batch 47.75 | train loss  4.35 | perplexity    77.53\n",
            "| Epoch  13 |   200/ 2983 batches | lr 5.00 | ms/batch 47.16 | train loss  4.33 | perplexity    76.13\n",
            "| Epoch  13 |   300/ 2983 batches | lr 5.00 | ms/batch 47.19 | train loss  4.35 | perplexity    77.23\n",
            "| Epoch  13 |   400/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.38 | perplexity    80.08\n",
            "| Epoch  13 |   500/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.23 | perplexity    68.50\n",
            "| Epoch  13 |   600/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.14 | perplexity    63.12\n",
            "| Epoch  13 |   700/ 2983 batches | lr 5.00 | ms/batch 47.14 | train loss  4.22 | perplexity    67.73\n",
            "| Epoch  13 |   800/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.30 | perplexity    73.40\n",
            "| Epoch  13 |   900/ 2983 batches | lr 5.00 | ms/batch 47.06 | train loss  4.28 | perplexity    72.58\n",
            "| Epoch  13 |  1000/ 2983 batches | lr 5.00 | ms/batch 47.14 | train loss  4.24 | perplexity    69.57\n",
            "| Epoch  13 |  1100/ 2983 batches | lr 5.00 | ms/batch 47.06 | train loss  4.24 | perplexity    69.61\n",
            "| Epoch  13 |  1200/ 2983 batches | lr 5.00 | ms/batch 46.95 | train loss  4.29 | perplexity    72.97\n",
            "| Epoch  13 |  1300/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.36 | perplexity    78.35\n",
            "| Epoch  13 |  1400/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.24 | perplexity    69.29\n",
            "| Epoch  13 |  1500/ 2983 batches | lr 5.00 | ms/batch 47.34 | train loss  4.33 | perplexity    75.94\n",
            "| Epoch  13 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.38 | perplexity    79.50\n",
            "| Epoch  13 |  1700/ 2983 batches | lr 5.00 | ms/batch 47.05 | train loss  4.24 | perplexity    69.34\n",
            "| Epoch  13 |  1800/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.27 | perplexity    71.17\n",
            "| Epoch  13 |  1900/ 2983 batches | lr 5.00 | ms/batch 47.03 | train loss  4.30 | perplexity    73.80\n",
            "| Epoch  13 |  2000/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.26 | perplexity    71.12\n",
            "| Epoch  13 |  2100/ 2983 batches | lr 5.00 | ms/batch 47.24 | train loss  4.20 | perplexity    66.71\n",
            "| Epoch  13 |  2200/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.14 | perplexity    62.49\n",
            "| Epoch  13 |  2300/ 2983 batches | lr 5.00 | ms/batch 47.19 | train loss  4.14 | perplexity    62.51\n",
            "| Epoch  13 |  2400/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.26 | perplexity    71.04\n",
            "| Epoch  13 |  2500/ 2983 batches | lr 5.00 | ms/batch 47.13 | train loss  4.17 | perplexity    64.98\n",
            "| Epoch  13 |  2600/ 2983 batches | lr 5.00 | ms/batch 47.21 | train loss  4.29 | perplexity    73.27\n",
            "| Epoch  13 |  2700/ 2983 batches | lr 5.00 | ms/batch 47.34 | train loss  4.22 | perplexity    67.78\n",
            "| Epoch  13 |  2800/ 2983 batches | lr 5.00 | ms/batch 47.04 | train loss  4.14 | perplexity    62.61\n",
            "| Epoch  13 |  2900/ 2983 batches | lr 5.00 | ms/batch 47.22 | train loss  4.15 | perplexity    63.50\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  12 | time: 144.97s | valid loss  4.94 | valid perplexity   140.03\n",
            "******************************************************************************************************\n",
            "| Epoch  14 |   100/ 2983 batches | lr 5.00 | ms/batch 47.55 | train loss  4.32 | perplexity    75.49\n",
            "| Epoch  14 |   200/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.30 | perplexity    73.59\n",
            "| Epoch  14 |   300/ 2983 batches | lr 5.00 | ms/batch 47.11 | train loss  4.32 | perplexity    75.11\n",
            "| Epoch  14 |   400/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.35 | perplexity    77.36\n",
            "| Epoch  14 |   500/ 2983 batches | lr 5.00 | ms/batch 47.01 | train loss  4.20 | perplexity    66.48\n",
            "| Epoch  14 |   600/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.13 | perplexity    61.92\n",
            "| Epoch  14 |   700/ 2983 batches | lr 5.00 | ms/batch 47.16 | train loss  4.19 | perplexity    66.02\n",
            "| Epoch  14 |   800/ 2983 batches | lr 5.00 | ms/batch 47.07 | train loss  4.27 | perplexity    71.24\n",
            "| Epoch  14 |   900/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.25 | perplexity    70.39\n",
            "| Epoch  14 |  1000/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.22 | perplexity    67.95\n",
            "| Epoch  14 |  1100/ 2983 batches | lr 5.00 | ms/batch 47.07 | train loss  4.22 | perplexity    67.73\n",
            "| Epoch  14 |  1200/ 2983 batches | lr 5.00 | ms/batch 46.95 | train loss  4.26 | perplexity    71.08\n",
            "| Epoch  14 |  1300/ 2983 batches | lr 5.00 | ms/batch 47.00 | train loss  4.33 | perplexity    76.00\n",
            "| Epoch  14 |  1400/ 2983 batches | lr 5.00 | ms/batch 47.27 | train loss  4.21 | perplexity    67.33\n",
            "| Epoch  14 |  1500/ 2983 batches | lr 5.00 | ms/batch 47.10 | train loss  4.31 | perplexity    74.61\n",
            "| Epoch  14 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.34 | train loss  4.35 | perplexity    77.59\n",
            "| Epoch  14 |  1700/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.22 | perplexity    68.23\n",
            "| Epoch  14 |  1800/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.25 | perplexity    69.93\n",
            "| Epoch  14 |  1900/ 2983 batches | lr 5.00 | ms/batch 47.18 | train loss  4.28 | perplexity    72.32\n",
            "| Epoch  14 |  2000/ 2983 batches | lr 5.00 | ms/batch 47.17 | train loss  4.24 | perplexity    69.12\n",
            "| Epoch  14 |  2100/ 2983 batches | lr 5.00 | ms/batch 47.20 | train loss  4.17 | perplexity    65.02\n",
            "| Epoch  14 |  2200/ 2983 batches | lr 5.00 | ms/batch 47.12 | train loss  4.11 | perplexity    60.92\n",
            "| Epoch  14 |  2300/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.11 | perplexity    60.76\n",
            "| Epoch  14 |  2400/ 2983 batches | lr 5.00 | ms/batch 47.16 | train loss  4.24 | perplexity    69.18\n",
            "| Epoch  14 |  2500/ 2983 batches | lr 5.00 | ms/batch 47.15 | train loss  4.16 | perplexity    63.82\n",
            "| Epoch  14 |  2600/ 2983 batches | lr 5.00 | ms/batch 46.97 | train loss  4.27 | perplexity    71.78\n",
            "| Epoch  14 |  2700/ 2983 batches | lr 5.00 | ms/batch 47.08 | train loss  4.20 | perplexity    66.82\n",
            "| Epoch  14 |  2800/ 2983 batches | lr 5.00 | ms/batch 47.09 | train loss  4.12 | perplexity    61.53\n",
            "| Epoch  14 |  2900/ 2983 batches | lr 5.00 | ms/batch 47.05 | train loss  4.12 | perplexity    61.87\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  13 | time: 144.85s | valid loss  4.95 | valid perplexity   141.08\n",
            "******************************************************************************************************\n",
            "| Epoch  15 |   100/ 2983 batches | lr 1.25 | ms/batch 47.70 | train loss  4.34 | perplexity    76.56\n",
            "| Epoch  15 |   200/ 2983 batches | lr 1.25 | ms/batch 47.05 | train loss  4.33 | perplexity    75.87\n",
            "| Epoch  15 |   300/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.35 | perplexity    77.52\n",
            "| Epoch  15 |   400/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.39 | perplexity    80.51\n",
            "| Epoch  15 |   500/ 2983 batches | lr 1.25 | ms/batch 47.01 | train loss  4.24 | perplexity    69.33\n",
            "| Epoch  15 |   600/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.16 | perplexity    64.15\n",
            "| Epoch  15 |   700/ 2983 batches | lr 1.25 | ms/batch 47.01 | train loss  4.25 | perplexity    69.90\n",
            "| Epoch  15 |   800/ 2983 batches | lr 1.25 | ms/batch 47.11 | train loss  4.31 | perplexity    74.21\n",
            "| Epoch  15 |   900/ 2983 batches | lr 1.25 | ms/batch 47.17 | train loss  4.27 | perplexity    71.62\n",
            "| Epoch  15 |  1000/ 2983 batches | lr 1.25 | ms/batch 46.95 | train loss  4.22 | perplexity    68.23\n",
            "| Epoch  15 |  1100/ 2983 batches | lr 1.25 | ms/batch 46.81 | train loss  4.22 | perplexity    68.33\n",
            "| Epoch  15 |  1200/ 2983 batches | lr 1.25 | ms/batch 46.74 | train loss  4.27 | perplexity    71.39\n",
            "| Epoch  15 |  1300/ 2983 batches | lr 1.25 | ms/batch 47.09 | train loss  4.33 | perplexity    75.68\n",
            "| Epoch  15 |  1400/ 2983 batches | lr 1.25 | ms/batch 47.12 | train loss  4.20 | perplexity    66.92\n",
            "| Epoch  15 |  1500/ 2983 batches | lr 1.25 | ms/batch 47.20 | train loss  4.30 | perplexity    73.44\n",
            "| Epoch  15 |  1600/ 2983 batches | lr 1.25 | ms/batch 47.24 | train loss  4.34 | perplexity    76.93\n",
            "| Epoch  15 |  1700/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.21 | perplexity    67.45\n",
            "| Epoch  15 |  1800/ 2983 batches | lr 1.25 | ms/batch 47.10 | train loss  4.23 | perplexity    68.48\n",
            "| Epoch  15 |  1900/ 2983 batches | lr 1.25 | ms/batch 47.20 | train loss  4.26 | perplexity    70.90\n",
            "| Epoch  15 |  2000/ 2983 batches | lr 1.25 | ms/batch 47.15 | train loss  4.22 | perplexity    68.28\n",
            "| Epoch  15 |  2100/ 2983 batches | lr 1.25 | ms/batch 47.12 | train loss  4.16 | perplexity    64.28\n",
            "| Epoch  15 |  2200/ 2983 batches | lr 1.25 | ms/batch 47.14 | train loss  4.11 | perplexity    60.82\n",
            "| Epoch  15 |  2300/ 2983 batches | lr 1.25 | ms/batch 47.19 | train loss  4.08 | perplexity    59.02\n",
            "| Epoch  15 |  2400/ 2983 batches | lr 1.25 | ms/batch 47.21 | train loss  4.20 | perplexity    66.74\n",
            "| Epoch  15 |  2500/ 2983 batches | lr 1.25 | ms/batch 47.21 | train loss  4.10 | perplexity    60.19\n",
            "| Epoch  15 |  2600/ 2983 batches | lr 1.25 | ms/batch 47.12 | train loss  4.24 | perplexity    69.09\n",
            "| Epoch  15 |  2700/ 2983 batches | lr 1.25 | ms/batch 47.11 | train loss  4.17 | perplexity    64.45\n",
            "| Epoch  15 |  2800/ 2983 batches | lr 1.25 | ms/batch 47.20 | train loss  4.09 | perplexity    59.59\n",
            "| Epoch  15 |  2900/ 2983 batches | lr 1.25 | ms/batch 47.06 | train loss  4.08 | perplexity    59.06\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  14 | time: 144.85s | valid loss  4.88 | valid perplexity   132.23\n",
            "******************************************************************************************************\n",
            "| Epoch  16 |   100/ 2983 batches | lr 1.25 | ms/batch 47.58 | train loss  4.30 | perplexity    74.06\n",
            "| Epoch  16 |   200/ 2983 batches | lr 1.25 | ms/batch 47.05 | train loss  4.29 | perplexity    72.71\n",
            "| Epoch  16 |   300/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.31 | perplexity    74.14\n",
            "| Epoch  16 |   400/ 2983 batches | lr 1.25 | ms/batch 47.20 | train loss  4.35 | perplexity    77.24\n",
            "| Epoch  16 |   500/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.20 | perplexity    66.82\n",
            "| Epoch  16 |   600/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.12 | perplexity    61.58\n",
            "| Epoch  16 |   700/ 2983 batches | lr 1.25 | ms/batch 47.15 | train loss  4.21 | perplexity    67.43\n",
            "| Epoch  16 |   800/ 2983 batches | lr 1.25 | ms/batch 47.24 | train loss  4.27 | perplexity    71.67\n",
            "| Epoch  16 |   900/ 2983 batches | lr 1.25 | ms/batch 47.09 | train loss  4.24 | perplexity    69.52\n",
            "| Epoch  16 |  1000/ 2983 batches | lr 1.25 | ms/batch 47.06 | train loss  4.20 | perplexity    66.42\n",
            "| Epoch  16 |  1100/ 2983 batches | lr 1.25 | ms/batch 47.04 | train loss  4.19 | perplexity    66.29\n",
            "| Epoch  16 |  1200/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.25 | perplexity    70.04\n",
            "| Epoch  16 |  1300/ 2983 batches | lr 1.25 | ms/batch 47.14 | train loss  4.30 | perplexity    74.01\n",
            "| Epoch  16 |  1400/ 2983 batches | lr 1.25 | ms/batch 47.05 | train loss  4.18 | perplexity    65.33\n",
            "| Epoch  16 |  1500/ 2983 batches | lr 1.25 | ms/batch 46.95 | train loss  4.28 | perplexity    72.24\n",
            "| Epoch  16 |  1600/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.33 | perplexity    76.11\n",
            "| Epoch  16 |  1700/ 2983 batches | lr 1.25 | ms/batch 47.01 | train loss  4.19 | perplexity    66.30\n",
            "| Epoch  16 |  1800/ 2983 batches | lr 1.25 | ms/batch 47.13 | train loss  4.21 | perplexity    67.41\n",
            "| Epoch  16 |  1900/ 2983 batches | lr 1.25 | ms/batch 47.14 | train loss  4.25 | perplexity    69.85\n",
            "| Epoch  16 |  2000/ 2983 batches | lr 1.25 | ms/batch 47.19 | train loss  4.21 | perplexity    67.47\n",
            "| Epoch  16 |  2100/ 2983 batches | lr 1.25 | ms/batch 47.17 | train loss  4.15 | perplexity    63.56\n",
            "| Epoch  16 |  2200/ 2983 batches | lr 1.25 | ms/batch 47.12 | train loss  4.09 | perplexity    59.86\n",
            "| Epoch  16 |  2300/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.07 | perplexity    58.41\n",
            "| Epoch  16 |  2400/ 2983 batches | lr 1.25 | ms/batch 47.23 | train loss  4.19 | perplexity    66.31\n",
            "| Epoch  16 |  2500/ 2983 batches | lr 1.25 | ms/batch 47.15 | train loss  4.09 | perplexity    59.98\n",
            "| Epoch  16 |  2600/ 2983 batches | lr 1.25 | ms/batch 47.04 | train loss  4.23 | perplexity    68.72\n",
            "| Epoch  16 |  2700/ 2983 batches | lr 1.25 | ms/batch 47.08 | train loss  4.17 | perplexity    64.85\n",
            "| Epoch  16 |  2800/ 2983 batches | lr 1.25 | ms/batch 47.14 | train loss  4.09 | perplexity    59.52\n",
            "| Epoch  16 |  2900/ 2983 batches | lr 1.25 | ms/batch 47.12 | train loss  4.09 | perplexity    59.48\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  15 | time: 144.84s | valid loss  4.89 | valid perplexity   132.41\n",
            "******************************************************************************************************\n",
            "| Epoch  17 |   100/ 2983 batches | lr 0.31 | ms/batch 47.64 | train loss  4.31 | perplexity    74.08\n",
            "| Epoch  17 |   200/ 2983 batches | lr 0.31 | ms/batch 47.22 | train loss  4.31 | perplexity    74.71\n",
            "| Epoch  17 |   300/ 2983 batches | lr 0.31 | ms/batch 46.98 | train loss  4.34 | perplexity    77.09\n",
            "| Epoch  17 |   400/ 2983 batches | lr 0.31 | ms/batch 47.17 | train loss  4.39 | perplexity    80.66\n",
            "| Epoch  17 |   500/ 2983 batches | lr 0.31 | ms/batch 47.17 | train loss  4.24 | perplexity    69.08\n",
            "| Epoch  17 |   600/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.16 | perplexity    64.14\n",
            "| Epoch  17 |   700/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.25 | perplexity    70.24\n",
            "| Epoch  17 |   800/ 2983 batches | lr 0.31 | ms/batch 47.16 | train loss  4.33 | perplexity    75.68\n",
            "| Epoch  17 |   900/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.27 | perplexity    71.77\n",
            "| Epoch  17 |  1000/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.23 | perplexity    68.63\n",
            "| Epoch  17 |  1100/ 2983 batches | lr 0.31 | ms/batch 46.93 | train loss  4.22 | perplexity    67.72\n",
            "| Epoch  17 |  1200/ 2983 batches | lr 0.31 | ms/batch 46.99 | train loss  4.28 | perplexity    72.18\n",
            "| Epoch  17 |  1300/ 2983 batches | lr 0.31 | ms/batch 47.25 | train loss  4.34 | perplexity    76.92\n",
            "| Epoch  17 |  1400/ 2983 batches | lr 0.31 | ms/batch 47.07 | train loss  4.21 | perplexity    67.17\n",
            "| Epoch  17 |  1500/ 2983 batches | lr 0.31 | ms/batch 47.22 | train loss  4.29 | perplexity    73.06\n",
            "| Epoch  17 |  1600/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.34 | perplexity    76.98\n",
            "| Epoch  17 |  1700/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.21 | perplexity    67.65\n",
            "| Epoch  17 |  1800/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.21 | perplexity    67.35\n",
            "| Epoch  17 |  1900/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.26 | perplexity    70.71\n",
            "| Epoch  17 |  2000/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.21 | perplexity    67.15\n",
            "| Epoch  17 |  2100/ 2983 batches | lr 0.31 | ms/batch 47.15 | train loss  4.15 | perplexity    63.48\n",
            "| Epoch  17 |  2200/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.10 | perplexity    60.36\n",
            "| Epoch  17 |  2300/ 2983 batches | lr 0.31 | ms/batch 47.17 | train loss  4.09 | perplexity    59.52\n",
            "| Epoch  17 |  2400/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.23 | perplexity    68.90\n",
            "| Epoch  17 |  2500/ 2983 batches | lr 0.31 | ms/batch 46.97 | train loss  4.10 | perplexity    60.57\n",
            "| Epoch  17 |  2600/ 2983 batches | lr 0.31 | ms/batch 47.04 | train loss  4.21 | perplexity    67.23\n",
            "| Epoch  17 |  2700/ 2983 batches | lr 0.31 | ms/batch 47.07 | train loss  4.16 | perplexity    63.93\n",
            "| Epoch  17 |  2800/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.08 | perplexity    59.43\n",
            "| Epoch  17 |  2900/ 2983 batches | lr 0.31 | ms/batch 47.15 | train loss  4.09 | perplexity    60.00\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  16 | time: 144.83s | valid loss  4.86 | valid perplexity   128.47\n",
            "******************************************************************************************************\n",
            "| Epoch  18 |   100/ 2983 batches | lr 0.31 | ms/batch 47.53 | train loss  4.33 | perplexity    75.66\n",
            "| Epoch  18 |   200/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.29 | perplexity    73.29\n",
            "| Epoch  18 |   300/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.32 | perplexity    75.31\n",
            "| Epoch  18 |   400/ 2983 batches | lr 0.31 | ms/batch 47.00 | train loss  4.36 | perplexity    78.37\n",
            "| Epoch  18 |   500/ 2983 batches | lr 0.31 | ms/batch 47.00 | train loss  4.22 | perplexity    67.76\n",
            "| Epoch  18 |   600/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.13 | perplexity    62.31\n",
            "| Epoch  18 |   700/ 2983 batches | lr 0.31 | ms/batch 47.18 | train loss  4.22 | perplexity    68.37\n",
            "| Epoch  18 |   800/ 2983 batches | lr 0.31 | ms/batch 47.00 | train loss  4.30 | perplexity    73.64\n",
            "| Epoch  18 |   900/ 2983 batches | lr 0.31 | ms/batch 47.15 | train loss  4.25 | perplexity    70.30\n",
            "| Epoch  18 |  1000/ 2983 batches | lr 0.31 | ms/batch 47.04 | train loss  4.21 | perplexity    67.32\n",
            "| Epoch  18 |  1100/ 2983 batches | lr 0.31 | ms/batch 46.92 | train loss  4.20 | perplexity    66.89\n",
            "| Epoch  18 |  1200/ 2983 batches | lr 0.31 | ms/batch 47.11 | train loss  4.26 | perplexity    71.15\n",
            "| Epoch  18 |  1300/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.32 | perplexity    75.43\n",
            "| Epoch  18 |  1400/ 2983 batches | lr 0.31 | ms/batch 46.98 | train loss  4.20 | perplexity    66.36\n",
            "| Epoch  18 |  1500/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.28 | perplexity    72.05\n",
            "| Epoch  18 |  1600/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.33 | perplexity    75.61\n",
            "| Epoch  18 |  1700/ 2983 batches | lr 0.31 | ms/batch 46.86 | train loss  4.21 | perplexity    67.30\n",
            "| Epoch  18 |  1800/ 2983 batches | lr 0.31 | ms/batch 46.92 | train loss  4.20 | perplexity    66.93\n",
            "| Epoch  18 |  1900/ 2983 batches | lr 0.31 | ms/batch 46.49 | train loss  4.25 | perplexity    70.22\n",
            "| Epoch  18 |  2000/ 2983 batches | lr 0.31 | ms/batch 46.98 | train loss  4.20 | perplexity    66.75\n",
            "| Epoch  18 |  2100/ 2983 batches | lr 0.31 | ms/batch 47.03 | train loss  4.15 | perplexity    63.29\n",
            "| Epoch  18 |  2200/ 2983 batches | lr 0.31 | ms/batch 46.93 | train loss  4.10 | perplexity    60.10\n",
            "| Epoch  18 |  2300/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.09 | perplexity    59.46\n",
            "| Epoch  18 |  2400/ 2983 batches | lr 0.31 | ms/batch 46.95 | train loss  4.23 | perplexity    68.52\n",
            "| Epoch  18 |  2500/ 2983 batches | lr 0.31 | ms/batch 47.09 | train loss  4.10 | perplexity    60.32\n",
            "| Epoch  18 |  2600/ 2983 batches | lr 0.31 | ms/batch 47.07 | train loss  4.21 | perplexity    67.40\n",
            "| Epoch  18 |  2700/ 2983 batches | lr 0.31 | ms/batch 46.98 | train loss  4.16 | perplexity    64.07\n",
            "| Epoch  18 |  2800/ 2983 batches | lr 0.31 | ms/batch 47.03 | train loss  4.09 | perplexity    59.58\n",
            "| Epoch  18 |  2900/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.10 | perplexity    60.30\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  17 | time: 144.56s | valid loss  4.85 | valid perplexity   128.19\n",
            "******************************************************************************************************\n",
            "| Epoch  19 |   100/ 2983 batches | lr 0.31 | ms/batch 47.62 | train loss  4.31 | perplexity    74.69\n",
            "| Epoch  19 |   200/ 2983 batches | lr 0.31 | ms/batch 47.04 | train loss  4.29 | perplexity    72.72\n",
            "| Epoch  19 |   300/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.32 | perplexity    74.83\n",
            "| Epoch  19 |   400/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.35 | perplexity    77.45\n",
            "| Epoch  19 |   500/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.20 | perplexity    67.00\n",
            "| Epoch  19 |   600/ 2983 batches | lr 0.31 | ms/batch 47.19 | train loss  4.12 | perplexity    61.66\n",
            "| Epoch  19 |   700/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.21 | perplexity    67.62\n",
            "| Epoch  19 |   800/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.29 | perplexity    72.83\n",
            "| Epoch  19 |   900/ 2983 batches | lr 0.31 | ms/batch 47.10 | train loss  4.25 | perplexity    69.88\n",
            "| Epoch  19 |  1000/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.20 | perplexity    66.95\n",
            "| Epoch  19 |  1100/ 2983 batches | lr 0.31 | ms/batch 46.87 | train loss  4.19 | perplexity    66.07\n",
            "| Epoch  19 |  1200/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.26 | perplexity    70.46\n",
            "| Epoch  19 |  1300/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.31 | perplexity    74.71\n",
            "| Epoch  19 |  1400/ 2983 batches | lr 0.31 | ms/batch 47.09 | train loss  4.19 | perplexity    65.96\n",
            "| Epoch  19 |  1500/ 2983 batches | lr 0.31 | ms/batch 47.03 | train loss  4.27 | perplexity    71.79\n",
            "| Epoch  19 |  1600/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.32 | perplexity    75.49\n",
            "| Epoch  19 |  1700/ 2983 batches | lr 0.31 | ms/batch 47.03 | train loss  4.21 | perplexity    67.09\n",
            "| Epoch  19 |  1800/ 2983 batches | lr 0.31 | ms/batch 46.71 | train loss  4.20 | perplexity    66.92\n",
            "| Epoch  19 |  1900/ 2983 batches | lr 0.31 | ms/batch 46.98 | train loss  4.25 | perplexity    69.82\n",
            "| Epoch  19 |  2000/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.20 | perplexity    66.86\n",
            "| Epoch  19 |  2100/ 2983 batches | lr 0.31 | ms/batch 47.26 | train loss  4.15 | perplexity    63.48\n",
            "| Epoch  19 |  2200/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.10 | perplexity    60.18\n",
            "| Epoch  19 |  2300/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.08 | perplexity    59.18\n",
            "| Epoch  19 |  2400/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.23 | perplexity    68.70\n",
            "| Epoch  19 |  2500/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.10 | perplexity    60.45\n",
            "| Epoch  19 |  2600/ 2983 batches | lr 0.31 | ms/batch 47.03 | train loss  4.21 | perplexity    67.38\n",
            "| Epoch  19 |  2700/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.16 | perplexity    64.12\n",
            "| Epoch  19 |  2800/ 2983 batches | lr 0.31 | ms/batch 47.11 | train loss  4.09 | perplexity    59.52\n",
            "| Epoch  19 |  2900/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.10 | perplexity    60.08\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  18 | time: 144.73s | valid loss  4.85 | valid perplexity   128.03\n",
            "******************************************************************************************************\n",
            "| Epoch  20 |   100/ 2983 batches | lr 0.31 | ms/batch 47.62 | train loss  4.31 | perplexity    74.55\n",
            "| Epoch  20 |   200/ 2983 batches | lr 0.31 | ms/batch 47.20 | train loss  4.28 | perplexity    72.46\n",
            "| Epoch  20 |   300/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.31 | perplexity    74.33\n",
            "| Epoch  20 |   400/ 2983 batches | lr 0.31 | ms/batch 47.07 | train loss  4.35 | perplexity    77.36\n",
            "| Epoch  20 |   500/ 2983 batches | lr 0.31 | ms/batch 47.21 | train loss  4.19 | perplexity    66.30\n",
            "| Epoch  20 |   600/ 2983 batches | lr 0.31 | ms/batch 47.00 | train loss  4.12 | perplexity    61.56\n",
            "| Epoch  20 |   700/ 2983 batches | lr 0.31 | ms/batch 47.24 | train loss  4.21 | perplexity    67.36\n",
            "| Epoch  20 |   800/ 2983 batches | lr 0.31 | ms/batch 47.22 | train loss  4.27 | perplexity    71.86\n",
            "| Epoch  20 |   900/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.24 | perplexity    69.39\n",
            "| Epoch  20 |  1000/ 2983 batches | lr 0.31 | ms/batch 47.07 | train loss  4.20 | perplexity    66.38\n",
            "| Epoch  20 |  1100/ 2983 batches | lr 0.31 | ms/batch 47.10 | train loss  4.18 | perplexity    65.68\n",
            "| Epoch  20 |  1200/ 2983 batches | lr 0.31 | ms/batch 47.26 | train loss  4.25 | perplexity    70.36\n",
            "| Epoch  20 |  1300/ 2983 batches | lr 0.31 | ms/batch 47.01 | train loss  4.31 | perplexity    74.35\n",
            "| Epoch  20 |  1400/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.18 | perplexity    65.43\n",
            "| Epoch  20 |  1500/ 2983 batches | lr 0.31 | ms/batch 47.17 | train loss  4.27 | perplexity    71.78\n",
            "| Epoch  20 |  1600/ 2983 batches | lr 0.31 | ms/batch 47.12 | train loss  4.32 | perplexity    75.33\n",
            "| Epoch  20 |  1700/ 2983 batches | lr 0.31 | ms/batch 47.09 | train loss  4.20 | perplexity    66.68\n",
            "| Epoch  20 |  1800/ 2983 batches | lr 0.31 | ms/batch 47.08 | train loss  4.20 | perplexity    66.61\n",
            "| Epoch  20 |  1900/ 2983 batches | lr 0.31 | ms/batch 47.10 | train loss  4.24 | perplexity    69.32\n",
            "| Epoch  20 |  2000/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.20 | perplexity    66.64\n",
            "| Epoch  20 |  2100/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.15 | perplexity    63.12\n",
            "| Epoch  20 |  2200/ 2983 batches | lr 0.31 | ms/batch 47.11 | train loss  4.09 | perplexity    59.83\n",
            "| Epoch  20 |  2300/ 2983 batches | lr 0.31 | ms/batch 47.16 | train loss  4.09 | perplexity    59.55\n",
            "| Epoch  20 |  2400/ 2983 batches | lr 0.31 | ms/batch 47.14 | train loss  4.22 | perplexity    68.18\n",
            "| Epoch  20 |  2500/ 2983 batches | lr 0.31 | ms/batch 47.13 | train loss  4.10 | perplexity    60.11\n",
            "| Epoch  20 |  2600/ 2983 batches | lr 0.31 | ms/batch 47.06 | train loss  4.21 | perplexity    67.58\n",
            "| Epoch  20 |  2700/ 2983 batches | lr 0.31 | ms/batch 47.15 | train loss  4.16 | perplexity    64.01\n",
            "| Epoch  20 |  2800/ 2983 batches | lr 0.31 | ms/batch 47.20 | train loss  4.09 | perplexity    59.63\n",
            "| Epoch  20 |  2900/ 2983 batches | lr 0.31 | ms/batch 47.05 | train loss  4.10 | perplexity    60.18\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  19 | time: 144.89s | valid loss  4.85 | valid perplexity   128.03\n",
            "******************************************************************************************************\n",
            "| Epoch  21 |   100/ 2983 batches | lr 0.08 | ms/batch 47.61 | train loss  4.32 | perplexity    74.97\n",
            "| Epoch  21 |   200/ 2983 batches | lr 0.08 | ms/batch 47.06 | train loss  4.29 | perplexity    72.72\n",
            "| Epoch  21 |   300/ 2983 batches | lr 0.08 | ms/batch 47.13 | train loss  4.33 | perplexity    75.83\n",
            "| Epoch  21 |   400/ 2983 batches | lr 0.08 | ms/batch 47.06 | train loss  4.36 | perplexity    78.49\n",
            "| Epoch  21 |   500/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.21 | perplexity    67.40\n",
            "| Epoch  21 |   600/ 2983 batches | lr 0.08 | ms/batch 47.03 | train loss  4.13 | perplexity    62.36\n",
            "| Epoch  21 |   700/ 2983 batches | lr 0.08 | ms/batch 47.28 | train loss  4.22 | perplexity    68.32\n",
            "| Epoch  21 |   800/ 2983 batches | lr 0.08 | ms/batch 46.98 | train loss  4.31 | perplexity    74.51\n",
            "| Epoch  21 |   900/ 2983 batches | lr 0.08 | ms/batch 47.05 | train loss  4.26 | perplexity    71.01\n",
            "| Epoch  21 |  1000/ 2983 batches | lr 0.08 | ms/batch 47.25 | train loss  4.21 | perplexity    67.69\n",
            "| Epoch  21 |  1100/ 2983 batches | lr 0.08 | ms/batch 46.98 | train loss  4.21 | perplexity    67.11\n",
            "| Epoch  21 |  1200/ 2983 batches | lr 0.08 | ms/batch 47.05 | train loss  4.27 | perplexity    71.83\n",
            "| Epoch  21 |  1300/ 2983 batches | lr 0.08 | ms/batch 47.04 | train loss  4.33 | perplexity    76.10\n",
            "| Epoch  21 |  1400/ 2983 batches | lr 0.08 | ms/batch 47.09 | train loss  4.21 | perplexity    67.12\n",
            "| Epoch  21 |  1500/ 2983 batches | lr 0.08 | ms/batch 47.17 | train loss  4.28 | perplexity    72.46\n",
            "| Epoch  21 |  1600/ 2983 batches | lr 0.08 | ms/batch 47.07 | train loss  4.33 | perplexity    75.85\n",
            "| Epoch  21 |  1700/ 2983 batches | lr 0.08 | ms/batch 47.15 | train loss  4.21 | perplexity    67.43\n",
            "| Epoch  21 |  1800/ 2983 batches | lr 0.08 | ms/batch 47.14 | train loss  4.20 | perplexity    66.84\n",
            "| Epoch  21 |  1900/ 2983 batches | lr 0.08 | ms/batch 46.97 | train loss  4.25 | perplexity    69.84\n",
            "| Epoch  21 |  2000/ 2983 batches | lr 0.08 | ms/batch 47.02 | train loss  4.19 | perplexity    66.32\n",
            "| Epoch  21 |  2100/ 2983 batches | lr 0.08 | ms/batch 47.05 | train loss  4.14 | perplexity    62.90\n",
            "| Epoch  21 |  2200/ 2983 batches | lr 0.08 | ms/batch 47.03 | train loss  4.08 | perplexity    59.35\n",
            "| Epoch  21 |  2300/ 2983 batches | lr 0.08 | ms/batch 47.19 | train loss  4.07 | perplexity    58.63\n",
            "| Epoch  21 |  2400/ 2983 batches | lr 0.08 | ms/batch 47.13 | train loss  4.22 | perplexity    68.36\n",
            "| Epoch  21 |  2500/ 2983 batches | lr 0.08 | ms/batch 47.14 | train loss  4.11 | perplexity    60.79\n",
            "| Epoch  21 |  2600/ 2983 batches | lr 0.08 | ms/batch 47.16 | train loss  4.21 | perplexity    67.36\n",
            "| Epoch  21 |  2700/ 2983 batches | lr 0.08 | ms/batch 47.03 | train loss  4.15 | perplexity    63.34\n",
            "| Epoch  21 |  2800/ 2983 batches | lr 0.08 | ms/batch 47.07 | train loss  4.07 | perplexity    58.43\n",
            "| Epoch  21 |  2900/ 2983 batches | lr 0.08 | ms/batch 47.07 | train loss  4.08 | perplexity    59.34\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  20 | time: 144.81s | valid loss  4.85 | valid perplexity   127.68\n",
            "******************************************************************************************************\n",
            "| Epoch  22 |   100/ 2983 batches | lr 0.08 | ms/batch 47.64 | train loss  4.33 | perplexity    75.66\n",
            "| Epoch  22 |   200/ 2983 batches | lr 0.08 | ms/batch 47.09 | train loss  4.28 | perplexity    72.37\n",
            "| Epoch  22 |   300/ 2983 batches | lr 0.08 | ms/batch 47.24 | train loss  4.32 | perplexity    75.09\n",
            "| Epoch  22 |   400/ 2983 batches | lr 0.08 | ms/batch 47.07 | train loss  4.35 | perplexity    77.68\n",
            "| Epoch  22 |   500/ 2983 batches | lr 0.08 | ms/batch 47.27 | train loss  4.21 | perplexity    67.07\n",
            "| Epoch  22 |   600/ 2983 batches | lr 0.08 | ms/batch 47.20 | train loss  4.13 | perplexity    62.08\n",
            "| Epoch  22 |   700/ 2983 batches | lr 0.08 | ms/batch 47.17 | train loss  4.22 | perplexity    67.95\n",
            "| Epoch  22 |   800/ 2983 batches | lr 0.08 | ms/batch 47.25 | train loss  4.30 | perplexity    73.38\n",
            "| Epoch  22 |   900/ 2983 batches | lr 0.08 | ms/batch 47.22 | train loss  4.25 | perplexity    70.24\n",
            "| Epoch  22 |  1000/ 2983 batches | lr 0.08 | ms/batch 46.97 | train loss  4.20 | perplexity    66.93\n",
            "| Epoch  22 |  1100/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.20 | perplexity    66.75\n",
            "| Epoch  22 |  1200/ 2983 batches | lr 0.08 | ms/batch 47.25 | train loss  4.27 | perplexity    71.35\n",
            "| Epoch  22 |  1300/ 2983 batches | lr 0.08 | ms/batch 47.13 | train loss  4.32 | perplexity    75.36\n",
            "| Epoch  22 |  1400/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.20 | perplexity    66.54\n",
            "| Epoch  22 |  1500/ 2983 batches | lr 0.08 | ms/batch 47.15 | train loss  4.28 | perplexity    72.20\n",
            "| Epoch  22 |  1600/ 2983 batches | lr 0.08 | ms/batch 47.11 | train loss  4.32 | perplexity    75.48\n",
            "| Epoch  22 |  1700/ 2983 batches | lr 0.08 | ms/batch 47.19 | train loss  4.21 | perplexity    67.27\n",
            "| Epoch  22 |  1800/ 2983 batches | lr 0.08 | ms/batch 47.17 | train loss  4.20 | perplexity    66.61\n",
            "| Epoch  22 |  1900/ 2983 batches | lr 0.08 | ms/batch 47.13 | train loss  4.24 | perplexity    69.72\n",
            "| Epoch  22 |  2000/ 2983 batches | lr 0.08 | ms/batch 47.18 | train loss  4.20 | perplexity    66.70\n",
            "| Epoch  22 |  2100/ 2983 batches | lr 0.08 | ms/batch 47.00 | train loss  4.15 | perplexity    63.20\n",
            "| Epoch  22 |  2200/ 2983 batches | lr 0.08 | ms/batch 47.09 | train loss  4.09 | perplexity    59.62\n",
            "| Epoch  22 |  2300/ 2983 batches | lr 0.08 | ms/batch 47.16 | train loss  4.08 | perplexity    59.14\n",
            "| Epoch  22 |  2400/ 2983 batches | lr 0.08 | ms/batch 47.10 | train loss  4.22 | perplexity    68.24\n",
            "| Epoch  22 |  2500/ 2983 batches | lr 0.08 | ms/batch 47.05 | train loss  4.11 | perplexity    61.06\n",
            "| Epoch  22 |  2600/ 2983 batches | lr 0.08 | ms/batch 47.09 | train loss  4.21 | perplexity    67.18\n",
            "| Epoch  22 |  2700/ 2983 batches | lr 0.08 | ms/batch 47.25 | train loss  4.15 | perplexity    63.63\n",
            "| Epoch  22 |  2800/ 2983 batches | lr 0.08 | ms/batch 47.24 | train loss  4.07 | perplexity    58.56\n",
            "| Epoch  22 |  2900/ 2983 batches | lr 0.08 | ms/batch 47.31 | train loss  4.09 | perplexity    59.71\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  21 | time: 144.98s | valid loss  4.85 | valid perplexity   127.51\n",
            "******************************************************************************************************\n",
            "| Epoch  23 |   100/ 2983 batches | lr 0.08 | ms/batch 47.60 | train loss  4.33 | perplexity    76.03\n",
            "| Epoch  23 |   200/ 2983 batches | lr 0.08 | ms/batch 47.14 | train loss  4.28 | perplexity    72.39\n",
            "| Epoch  23 |   300/ 2983 batches | lr 0.08 | ms/batch 47.28 | train loss  4.32 | perplexity    74.87\n",
            "| Epoch  23 |   400/ 2983 batches | lr 0.08 | ms/batch 47.01 | train loss  4.35 | perplexity    77.86\n",
            "| Epoch  23 |   500/ 2983 batches | lr 0.08 | ms/batch 47.10 | train loss  4.19 | perplexity    66.23\n",
            "| Epoch  23 |   600/ 2983 batches | lr 0.08 | ms/batch 47.21 | train loss  4.12 | perplexity    61.46\n",
            "| Epoch  23 |   700/ 2983 batches | lr 0.08 | ms/batch 47.20 | train loss  4.22 | perplexity    67.93\n",
            "| Epoch  23 |   800/ 2983 batches | lr 0.08 | ms/batch 47.19 | train loss  4.30 | perplexity    73.49\n",
            "| Epoch  23 |   900/ 2983 batches | lr 0.08 | ms/batch 47.26 | train loss  4.25 | perplexity    70.35\n",
            "| Epoch  23 |  1000/ 2983 batches | lr 0.08 | ms/batch 47.20 | train loss  4.20 | perplexity    66.96\n",
            "| Epoch  23 |  1100/ 2983 batches | lr 0.08 | ms/batch 46.96 | train loss  4.19 | perplexity    66.21\n",
            "| Epoch  23 |  1200/ 2983 batches | lr 0.08 | ms/batch 47.06 | train loss  4.26 | perplexity    70.87\n",
            "| Epoch  23 |  1300/ 2983 batches | lr 0.08 | ms/batch 47.09 | train loss  4.32 | perplexity    75.38\n",
            "| Epoch  23 |  1400/ 2983 batches | lr 0.08 | ms/batch 47.21 | train loss  4.20 | perplexity    66.37\n",
            "| Epoch  23 |  1500/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.27 | perplexity    71.67\n",
            "| Epoch  23 |  1600/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.32 | perplexity    75.18\n",
            "| Epoch  23 |  1700/ 2983 batches | lr 0.08 | ms/batch 47.30 | train loss  4.20 | perplexity    67.01\n",
            "| Epoch  23 |  1800/ 2983 batches | lr 0.08 | ms/batch 47.18 | train loss  4.20 | perplexity    66.50\n",
            "| Epoch  23 |  1900/ 2983 batches | lr 0.08 | ms/batch 47.14 | train loss  4.25 | perplexity    69.91\n",
            "| Epoch  23 |  2000/ 2983 batches | lr 0.08 | ms/batch 47.32 | train loss  4.19 | perplexity    66.34\n",
            "| Epoch  23 |  2100/ 2983 batches | lr 0.08 | ms/batch 47.14 | train loss  4.15 | perplexity    63.25\n",
            "| Epoch  23 |  2200/ 2983 batches | lr 0.08 | ms/batch 47.12 | train loss  4.09 | perplexity    59.45\n",
            "| Epoch  23 |  2300/ 2983 batches | lr 0.08 | ms/batch 46.94 | train loss  4.08 | perplexity    59.04\n",
            "| Epoch  23 |  2400/ 2983 batches | lr 0.08 | ms/batch 46.75 | train loss  4.22 | perplexity    68.09\n",
            "| Epoch  23 |  2500/ 2983 batches | lr 0.08 | ms/batch 47.27 | train loss  4.11 | perplexity    60.97\n",
            "| Epoch  23 |  2600/ 2983 batches | lr 0.08 | ms/batch 47.28 | train loss  4.21 | perplexity    67.41\n",
            "| Epoch  23 |  2700/ 2983 batches | lr 0.08 | ms/batch 47.21 | train loss  4.16 | perplexity    63.86\n",
            "| Epoch  23 |  2800/ 2983 batches | lr 0.08 | ms/batch 47.26 | train loss  4.07 | perplexity    58.73\n",
            "| Epoch  23 |  2900/ 2983 batches | lr 0.08 | ms/batch 47.28 | train loss  4.09 | perplexity    59.71\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  22 | time: 144.99s | valid loss  4.85 | valid perplexity   127.52\n",
            "******************************************************************************************************\n",
            "| Epoch  24 |   100/ 2983 batches | lr 0.02 | ms/batch 47.74 | train loss  4.33 | perplexity    75.77\n",
            "| Epoch  24 |   200/ 2983 batches | lr 0.02 | ms/batch 47.23 | train loss  4.28 | perplexity    72.55\n",
            "| Epoch  24 |   300/ 2983 batches | lr 0.02 | ms/batch 47.11 | train loss  4.32 | perplexity    74.93\n",
            "| Epoch  24 |   400/ 2983 batches | lr 0.02 | ms/batch 47.25 | train loss  4.36 | perplexity    78.00\n",
            "| Epoch  24 |   500/ 2983 batches | lr 0.02 | ms/batch 47.17 | train loss  4.20 | perplexity    66.65\n",
            "| Epoch  24 |   600/ 2983 batches | lr 0.02 | ms/batch 47.22 | train loss  4.13 | perplexity    61.88\n",
            "| Epoch  24 |   700/ 2983 batches | lr 0.02 | ms/batch 47.31 | train loss  4.22 | perplexity    67.88\n",
            "| Epoch  24 |   800/ 2983 batches | lr 0.02 | ms/batch 47.18 | train loss  4.31 | perplexity    74.27\n",
            "| Epoch  24 |   900/ 2983 batches | lr 0.02 | ms/batch 47.10 | train loss  4.26 | perplexity    71.15\n",
            "| Epoch  24 |  1000/ 2983 batches | lr 0.02 | ms/batch 47.34 | train loss  4.21 | perplexity    67.34\n",
            "| Epoch  24 |  1100/ 2983 batches | lr 0.02 | ms/batch 47.15 | train loss  4.20 | perplexity    66.70\n",
            "| Epoch  24 |  1200/ 2983 batches | lr 0.02 | ms/batch 47.17 | train loss  4.26 | perplexity    71.02\n",
            "| Epoch  24 |  1300/ 2983 batches | lr 0.02 | ms/batch 47.09 | train loss  4.32 | perplexity    75.38\n",
            "| Epoch  24 |  1400/ 2983 batches | lr 0.02 | ms/batch 47.17 | train loss  4.20 | perplexity    66.70\n",
            "| Epoch  24 |  1500/ 2983 batches | lr 0.02 | ms/batch 47.11 | train loss  4.28 | perplexity    72.14\n",
            "| Epoch  24 |  1600/ 2983 batches | lr 0.02 | ms/batch 47.26 | train loss  4.32 | perplexity    75.08\n",
            "| Epoch  24 |  1700/ 2983 batches | lr 0.02 | ms/batch 47.12 | train loss  4.21 | perplexity    67.18\n",
            "| Epoch  24 |  1800/ 2983 batches | lr 0.02 | ms/batch 47.23 | train loss  4.20 | perplexity    66.60\n",
            "| Epoch  24 |  1900/ 2983 batches | lr 0.02 | ms/batch 47.22 | train loss  4.25 | perplexity    69.92\n",
            "| Epoch  24 |  2000/ 2983 batches | lr 0.02 | ms/batch 47.12 | train loss  4.20 | perplexity    66.69\n",
            "| Epoch  24 |  2100/ 2983 batches | lr 0.02 | ms/batch 47.09 | train loss  4.14 | perplexity    63.03\n",
            "| Epoch  24 |  2200/ 2983 batches | lr 0.02 | ms/batch 47.38 | train loss  4.08 | perplexity    59.33\n",
            "| Epoch  24 |  2300/ 2983 batches | lr 0.02 | ms/batch 47.14 | train loss  4.08 | perplexity    58.97\n",
            "| Epoch  24 |  2400/ 2983 batches | lr 0.02 | ms/batch 47.18 | train loss  4.22 | perplexity    68.08\n",
            "| Epoch  24 |  2500/ 2983 batches | lr 0.02 | ms/batch 47.24 | train loss  4.11 | perplexity    61.06\n",
            "| Epoch  24 |  2600/ 2983 batches | lr 0.02 | ms/batch 47.12 | train loss  4.21 | perplexity    67.11\n",
            "| Epoch  24 |  2700/ 2983 batches | lr 0.02 | ms/batch 47.07 | train loss  4.15 | perplexity    63.21\n",
            "| Epoch  24 |  2800/ 2983 batches | lr 0.02 | ms/batch 46.98 | train loss  4.06 | perplexity    58.22\n",
            "| Epoch  24 |  2900/ 2983 batches | lr 0.02 | ms/batch 46.76 | train loss  4.08 | perplexity    59.03\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  23 | time: 145.02s | valid loss  4.85 | valid perplexity   127.56\n",
            "******************************************************************************************************\n",
            "| Epoch  25 |   100/ 2983 batches | lr 0.00 | ms/batch 47.60 | train loss  4.33 | perplexity    75.93\n",
            "| Epoch  25 |   200/ 2983 batches | lr 0.00 | ms/batch 47.18 | train loss  4.28 | perplexity    72.30\n",
            "| Epoch  25 |   300/ 2983 batches | lr 0.00 | ms/batch 46.98 | train loss  4.32 | perplexity    75.02\n",
            "| Epoch  25 |   400/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.36 | perplexity    78.21\n",
            "| Epoch  25 |   500/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.20 | perplexity    66.60\n",
            "| Epoch  25 |   600/ 2983 batches | lr 0.00 | ms/batch 47.11 | train loss  4.12 | perplexity    61.74\n",
            "| Epoch  25 |   700/ 2983 batches | lr 0.00 | ms/batch 47.12 | train loss  4.22 | perplexity    68.14\n",
            "| Epoch  25 |   800/ 2983 batches | lr 0.00 | ms/batch 47.27 | train loss  4.30 | perplexity    73.80\n",
            "| Epoch  25 |   900/ 2983 batches | lr 0.00 | ms/batch 47.12 | train loss  4.26 | perplexity    70.67\n",
            "| Epoch  25 |  1000/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.20 | perplexity    66.97\n",
            "| Epoch  25 |  1100/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.20 | perplexity    66.85\n",
            "| Epoch  25 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.01 | train loss  4.26 | perplexity    70.97\n",
            "| Epoch  25 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.33 | perplexity    75.61\n",
            "| Epoch  25 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.20 | perplexity    66.77\n",
            "| Epoch  25 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.27 | train loss  4.28 | perplexity    72.04\n",
            "| Epoch  25 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.15 | train loss  4.33 | perplexity    75.75\n",
            "| Epoch  25 |  1700/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.21 | perplexity    67.27\n",
            "| Epoch  25 |  1800/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.20 | perplexity    66.74\n",
            "| Epoch  25 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.29 | train loss  4.25 | perplexity    70.13\n",
            "| Epoch  25 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.28 | train loss  4.20 | perplexity    66.53\n",
            "| Epoch  25 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.14 | perplexity    62.80\n",
            "| Epoch  25 |  2200/ 2983 batches | lr 0.00 | ms/batch 47.10 | train loss  4.08 | perplexity    58.99\n",
            "| Epoch  25 |  2300/ 2983 batches | lr 0.00 | ms/batch 47.17 | train loss  4.07 | perplexity    58.84\n",
            "| Epoch  25 |  2400/ 2983 batches | lr 0.00 | ms/batch 46.94 | train loss  4.22 | perplexity    67.95\n",
            "| Epoch  25 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.26 | train loss  4.11 | perplexity    60.88\n",
            "| Epoch  25 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.24 | train loss  4.21 | perplexity    67.23\n",
            "| Epoch  25 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.11 | train loss  4.15 | perplexity    63.38\n",
            "| Epoch  25 |  2800/ 2983 batches | lr 0.00 | ms/batch 46.92 | train loss  4.07 | perplexity    58.58\n",
            "| Epoch  25 |  2900/ 2983 batches | lr 0.00 | ms/batch 46.89 | train loss  4.08 | perplexity    59.26\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  24 | time: 144.96s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  26 |   100/ 2983 batches | lr 0.00 | ms/batch 47.69 | train loss  4.33 | perplexity    75.90\n",
            "| Epoch  26 |   200/ 2983 batches | lr 0.00 | ms/batch 47.04 | train loss  4.29 | perplexity    72.66\n",
            "| Epoch  26 |   300/ 2983 batches | lr 0.00 | ms/batch 46.91 | train loss  4.32 | perplexity    74.93\n",
            "| Epoch  26 |   400/ 2983 batches | lr 0.00 | ms/batch 46.81 | train loss  4.36 | perplexity    78.31\n",
            "| Epoch  26 |   500/ 2983 batches | lr 0.00 | ms/batch 46.91 | train loss  4.20 | perplexity    66.79\n",
            "| Epoch  26 |   600/ 2983 batches | lr 0.00 | ms/batch 46.97 | train loss  4.13 | perplexity    61.94\n",
            "| Epoch  26 |   700/ 2983 batches | lr 0.00 | ms/batch 47.20 | train loss  4.22 | perplexity    67.97\n",
            "| Epoch  26 |   800/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.31 | perplexity    74.09\n",
            "| Epoch  26 |   900/ 2983 batches | lr 0.00 | ms/batch 46.83 | train loss  4.26 | perplexity    70.78\n",
            "| Epoch  26 |  1000/ 2983 batches | lr 0.00 | ms/batch 46.91 | train loss  4.21 | perplexity    67.39\n",
            "| Epoch  26 |  1100/ 2983 batches | lr 0.00 | ms/batch 46.84 | train loss  4.20 | perplexity    66.68\n",
            "| Epoch  26 |  1200/ 2983 batches | lr 0.00 | ms/batch 46.90 | train loss  4.27 | perplexity    71.49\n",
            "| Epoch  26 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.32 | perplexity    75.25\n",
            "| Epoch  26 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.20 | perplexity    66.75\n",
            "| Epoch  26 |  1500/ 2983 batches | lr 0.00 | ms/batch 46.89 | train loss  4.28 | perplexity    72.24\n",
            "| Epoch  26 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.01 | train loss  4.32 | perplexity    75.41\n",
            "| Epoch  26 |  1700/ 2983 batches | lr 0.00 | ms/batch 46.88 | train loss  4.20 | perplexity    66.82\n",
            "| Epoch  26 |  1800/ 2983 batches | lr 0.00 | ms/batch 46.99 | train loss  4.20 | perplexity    66.50\n",
            "| Epoch  26 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.25 | perplexity    69.93\n",
            "| Epoch  26 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.19 | perplexity    66.29\n",
            "| Epoch  26 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.03 | train loss  4.15 | perplexity    63.28\n",
            "| Epoch  26 |  2200/ 2983 batches | lr 0.00 | ms/batch 46.98 | train loss  4.08 | perplexity    59.16\n",
            "| Epoch  26 |  2300/ 2983 batches | lr 0.00 | ms/batch 46.97 | train loss  4.08 | perplexity    58.88\n",
            "| Epoch  26 |  2400/ 2983 batches | lr 0.00 | ms/batch 46.96 | train loss  4.22 | perplexity    67.79\n",
            "| Epoch  26 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.11 | perplexity    61.00\n",
            "| Epoch  26 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.20 | perplexity    66.88\n",
            "| Epoch  26 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.04 | train loss  4.15 | perplexity    63.70\n",
            "| Epoch  26 |  2800/ 2983 batches | lr 0.00 | ms/batch 46.87 | train loss  4.06 | perplexity    58.10\n",
            "| Epoch  26 |  2900/ 2983 batches | lr 0.00 | ms/batch 46.97 | train loss  4.08 | perplexity    59.21\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  25 | time: 144.58s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  27 |   100/ 2983 batches | lr 0.00 | ms/batch 47.68 | train loss  4.33 | perplexity    76.06\n",
            "| Epoch  27 |   200/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.29 | perplexity    72.78\n",
            "| Epoch  27 |   300/ 2983 batches | lr 0.00 | ms/batch 47.01 | train loss  4.32 | perplexity    75.40\n",
            "| Epoch  27 |   400/ 2983 batches | lr 0.00 | ms/batch 47.15 | train loss  4.36 | perplexity    78.07\n",
            "| Epoch  27 |   500/ 2983 batches | lr 0.00 | ms/batch 46.97 | train loss  4.20 | perplexity    66.95\n",
            "| Epoch  27 |   600/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.12 | perplexity    61.68\n",
            "| Epoch  27 |   700/ 2983 batches | lr 0.00 | ms/batch 47.33 | train loss  4.21 | perplexity    67.68\n",
            "| Epoch  27 |   800/ 2983 batches | lr 0.00 | ms/batch 47.29 | train loss  4.30 | perplexity    74.04\n",
            "| Epoch  27 |   900/ 2983 batches | lr 0.00 | ms/batch 47.11 | train loss  4.26 | perplexity    70.90\n",
            "| Epoch  27 |  1000/ 2983 batches | lr 0.00 | ms/batch 46.99 | train loss  4.20 | perplexity    66.94\n",
            "| Epoch  27 |  1100/ 2983 batches | lr 0.00 | ms/batch 46.96 | train loss  4.20 | perplexity    66.71\n",
            "| Epoch  27 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.27 | perplexity    71.38\n",
            "| Epoch  27 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.30 | train loss  4.32 | perplexity    75.39\n",
            "| Epoch  27 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.33 | train loss  4.20 | perplexity    66.81\n",
            "| Epoch  27 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.04 | train loss  4.28 | perplexity    71.99\n",
            "| Epoch  27 |  1600/ 2983 batches | lr 0.00 | ms/batch 46.96 | train loss  4.32 | perplexity    75.25\n",
            "| Epoch  27 |  1700/ 2983 batches | lr 0.00 | ms/batch 47.03 | train loss  4.20 | perplexity    66.94\n",
            "| Epoch  27 |  1800/ 2983 batches | lr 0.00 | ms/batch 47.19 | train loss  4.20 | perplexity    66.67\n",
            "| Epoch  27 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.26 | train loss  4.25 | perplexity    70.03\n",
            "| Epoch  27 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.34 | train loss  4.20 | perplexity    66.36\n",
            "| Epoch  27 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.14 | perplexity    62.73\n",
            "| Epoch  27 |  2200/ 2983 batches | lr 0.00 | ms/batch 46.95 | train loss  4.08 | perplexity    59.29\n",
            "| Epoch  27 |  2300/ 2983 batches | lr 0.00 | ms/batch 47.42 | train loss  4.07 | perplexity    58.44\n",
            "| Epoch  27 |  2400/ 2983 batches | lr 0.00 | ms/batch 47.15 | train loss  4.22 | perplexity    67.79\n",
            "| Epoch  27 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.22 | train loss  4.11 | perplexity    61.04\n",
            "| Epoch  27 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.32 | train loss  4.21 | perplexity    67.11\n",
            "| Epoch  27 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.15 | perplexity    63.23\n",
            "| Epoch  27 |  2800/ 2983 batches | lr 0.00 | ms/batch 47.36 | train loss  4.06 | perplexity    58.25\n",
            "| Epoch  27 |  2900/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.08 | perplexity    58.94\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  26 | time: 145.10s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  28 |   100/ 2983 batches | lr 0.00 | ms/batch 47.93 | train loss  4.33 | perplexity    75.94\n",
            "| Epoch  28 |   200/ 2983 batches | lr 0.00 | ms/batch 47.30 | train loss  4.29 | perplexity    72.68\n",
            "| Epoch  28 |   300/ 2983 batches | lr 0.00 | ms/batch 47.41 | train loss  4.32 | perplexity    74.89\n",
            "| Epoch  28 |   400/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.36 | perplexity    78.25\n",
            "| Epoch  28 |   500/ 2983 batches | lr 0.00 | ms/batch 47.30 | train loss  4.21 | perplexity    67.07\n",
            "| Epoch  28 |   600/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.12 | perplexity    61.79\n",
            "| Epoch  28 |   700/ 2983 batches | lr 0.00 | ms/batch 47.48 | train loss  4.22 | perplexity    67.83\n",
            "| Epoch  28 |   800/ 2983 batches | lr 0.00 | ms/batch 47.31 | train loss  4.30 | perplexity    73.39\n",
            "| Epoch  28 |   900/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.26 | perplexity    71.09\n",
            "| Epoch  28 |  1000/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.21 | perplexity    67.56\n",
            "| Epoch  28 |  1100/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.20 | perplexity    66.57\n",
            "| Epoch  28 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.27 | perplexity    71.40\n",
            "| Epoch  28 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.46 | train loss  4.32 | perplexity    75.43\n",
            "| Epoch  28 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.40 | train loss  4.20 | perplexity    66.81\n",
            "| Epoch  28 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.28 | perplexity    72.27\n",
            "| Epoch  28 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.28 | train loss  4.32 | perplexity    75.16\n",
            "| Epoch  28 |  1700/ 2983 batches | lr 0.00 | ms/batch 47.22 | train loss  4.21 | perplexity    67.07\n",
            "| Epoch  28 |  1800/ 2983 batches | lr 0.00 | ms/batch 47.36 | train loss  4.20 | perplexity    66.88\n",
            "| Epoch  28 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.43 | train loss  4.25 | perplexity    69.82\n",
            "| Epoch  28 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.34 | train loss  4.19 | perplexity    66.26\n",
            "| Epoch  28 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.29 | train loss  4.14 | perplexity    62.86\n",
            "| Epoch  28 |  2200/ 2983 batches | lr 0.00 | ms/batch 47.32 | train loss  4.08 | perplexity    59.34\n",
            "| Epoch  28 |  2300/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.07 | perplexity    58.44\n",
            "| Epoch  28 |  2400/ 2983 batches | lr 0.00 | ms/batch 47.35 | train loss  4.21 | perplexity    67.61\n",
            "| Epoch  28 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.46 | train loss  4.11 | perplexity    61.12\n",
            "| Epoch  28 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.20 | perplexity    66.88\n",
            "| Epoch  28 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.15 | train loss  4.15 | perplexity    63.32\n",
            "| Epoch  28 |  2800/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.07 | perplexity    58.39\n",
            "| Epoch  28 |  2900/ 2983 batches | lr 0.00 | ms/batch 47.36 | train loss  4.07 | perplexity    58.81\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  27 | time: 145.52s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  29 |   100/ 2983 batches | lr 0.00 | ms/batch 48.01 | train loss  4.33 | perplexity    76.00\n",
            "| Epoch  29 |   200/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.29 | perplexity    72.85\n",
            "| Epoch  29 |   300/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.32 | perplexity    75.04\n",
            "| Epoch  29 |   400/ 2983 batches | lr 0.00 | ms/batch 47.35 | train loss  4.35 | perplexity    77.83\n",
            "| Epoch  29 |   500/ 2983 batches | lr 0.00 | ms/batch 47.19 | train loss  4.21 | perplexity    67.03\n",
            "| Epoch  29 |   600/ 2983 batches | lr 0.00 | ms/batch 47.48 | train loss  4.12 | perplexity    61.79\n",
            "| Epoch  29 |   700/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.22 | perplexity    68.02\n",
            "| Epoch  29 |   800/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.29 | perplexity    73.24\n",
            "| Epoch  29 |   900/ 2983 batches | lr 0.00 | ms/batch 47.34 | train loss  4.26 | perplexity    71.07\n",
            "| Epoch  29 |  1000/ 2983 batches | lr 0.00 | ms/batch 47.19 | train loss  4.21 | perplexity    67.46\n",
            "| Epoch  29 |  1100/ 2983 batches | lr 0.00 | ms/batch 47.33 | train loss  4.20 | perplexity    66.62\n",
            "| Epoch  29 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.27 | perplexity    71.18\n",
            "| Epoch  29 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.41 | train loss  4.33 | perplexity    75.58\n",
            "| Epoch  29 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.24 | train loss  4.20 | perplexity    66.88\n",
            "| Epoch  29 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.17 | train loss  4.28 | perplexity    71.95\n",
            "| Epoch  29 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.32 | perplexity    75.20\n",
            "| Epoch  29 |  1700/ 2983 batches | lr 0.00 | ms/batch 47.30 | train loss  4.20 | perplexity    66.77\n",
            "| Epoch  29 |  1800/ 2983 batches | lr 0.00 | ms/batch 47.47 | train loss  4.20 | perplexity    66.45\n",
            "| Epoch  29 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.53 | train loss  4.25 | perplexity    70.05\n",
            "| Epoch  29 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.40 | train loss  4.19 | perplexity    66.33\n",
            "| Epoch  29 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.37 | train loss  4.14 | perplexity    62.98\n",
            "| Epoch  29 |  2200/ 2983 batches | lr 0.00 | ms/batch 47.36 | train loss  4.08 | perplexity    59.10\n",
            "| Epoch  29 |  2300/ 2983 batches | lr 0.00 | ms/batch 47.27 | train loss  4.07 | perplexity    58.69\n",
            "| Epoch  29 |  2400/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.22 | perplexity    67.70\n",
            "| Epoch  29 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.45 | train loss  4.11 | perplexity    60.83\n",
            "| Epoch  29 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.55 | train loss  4.20 | perplexity    66.98\n",
            "| Epoch  29 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.38 | train loss  4.15 | perplexity    63.37\n",
            "| Epoch  29 |  2800/ 2983 batches | lr 0.00 | ms/batch 47.29 | train loss  4.06 | perplexity    58.08\n",
            "| Epoch  29 |  2900/ 2983 batches | lr 0.00 | ms/batch 47.39 | train loss  4.08 | perplexity    59.40\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  28 | time: 145.61s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  30 |   100/ 2983 batches | lr 0.00 | ms/batch 47.94 | train loss  4.33 | perplexity    76.02\n",
            "| Epoch  30 |   200/ 2983 batches | lr 0.00 | ms/batch 47.31 | train loss  4.28 | perplexity    72.53\n",
            "| Epoch  30 |   300/ 2983 batches | lr 0.00 | ms/batch 47.16 | train loss  4.32 | perplexity    75.05\n",
            "| Epoch  30 |   400/ 2983 batches | lr 0.00 | ms/batch 47.12 | train loss  4.36 | perplexity    78.42\n",
            "| Epoch  30 |   500/ 2983 batches | lr 0.00 | ms/batch 47.26 | train loss  4.20 | perplexity    66.67\n",
            "| Epoch  30 |   600/ 2983 batches | lr 0.00 | ms/batch 47.30 | train loss  4.12 | perplexity    61.82\n",
            "| Epoch  30 |   700/ 2983 batches | lr 0.00 | ms/batch 47.40 | train loss  4.22 | perplexity    67.73\n",
            "| Epoch  30 |   800/ 2983 batches | lr 0.00 | ms/batch 47.28 | train loss  4.31 | perplexity    74.33\n",
            "| Epoch  30 |   900/ 2983 batches | lr 0.00 | ms/batch 47.22 | train loss  4.26 | perplexity    70.66\n",
            "| Epoch  30 |  1000/ 2983 batches | lr 0.00 | ms/batch 47.24 | train loss  4.21 | perplexity    67.55\n",
            "| Epoch  30 |  1100/ 2983 batches | lr 0.00 | ms/batch 47.31 | train loss  4.20 | perplexity    66.94\n",
            "| Epoch  30 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.39 | train loss  4.27 | perplexity    71.37\n",
            "| Epoch  30 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.44 | train loss  4.32 | perplexity    75.56\n",
            "| Epoch  30 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.31 | train loss  4.20 | perplexity    66.86\n",
            "| Epoch  30 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.28 | perplexity    72.06\n",
            "| Epoch  30 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.20 | train loss  4.32 | perplexity    75.22\n",
            "| Epoch  30 |  1700/ 2983 batches | lr 0.00 | ms/batch 47.25 | train loss  4.21 | perplexity    67.28\n",
            "| Epoch  30 |  1800/ 2983 batches | lr 0.00 | ms/batch 47.39 | train loss  4.21 | perplexity    67.23\n",
            "| Epoch  30 |  1900/ 2983 batches | lr 0.00 | ms/batch 47.34 | train loss  4.25 | perplexity    69.85\n",
            "| Epoch  30 |  2000/ 2983 batches | lr 0.00 | ms/batch 47.20 | train loss  4.19 | perplexity    66.05\n",
            "| Epoch  30 |  2100/ 2983 batches | lr 0.00 | ms/batch 47.28 | train loss  4.14 | perplexity    62.74\n",
            "| Epoch  30 |  2200/ 2983 batches | lr 0.00 | ms/batch 47.31 | train loss  4.08 | perplexity    59.01\n",
            "| Epoch  30 |  2300/ 2983 batches | lr 0.00 | ms/batch 47.21 | train loss  4.07 | perplexity    58.63\n",
            "| Epoch  30 |  2400/ 2983 batches | lr 0.00 | ms/batch 47.11 | train loss  4.22 | perplexity    67.97\n",
            "| Epoch  30 |  2500/ 2983 batches | lr 0.00 | ms/batch 47.35 | train loss  4.11 | perplexity    61.08\n",
            "| Epoch  30 |  2600/ 2983 batches | lr 0.00 | ms/batch 47.34 | train loss  4.20 | perplexity    66.84\n",
            "| Epoch  30 |  2700/ 2983 batches | lr 0.00 | ms/batch 47.40 | train loss  4.15 | perplexity    63.37\n",
            "| Epoch  30 |  2800/ 2983 batches | lr 0.00 | ms/batch 47.35 | train loss  4.07 | perplexity    58.38\n",
            "| Epoch  30 |  2900/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.08 | perplexity    58.98\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  29 | time: 145.42s | valid loss  4.85 | valid perplexity   127.55\n",
            "******************************************************************************************************\n",
            "| Epoch  31 |   100/ 2983 batches | lr 0.00 | ms/batch 47.84 | train loss  4.33 | perplexity    75.95\n",
            "| Epoch  31 |   200/ 2983 batches | lr 0.00 | ms/batch 47.15 | train loss  4.28 | perplexity    72.51\n",
            "| Epoch  31 |   300/ 2983 batches | lr 0.00 | ms/batch 47.23 | train loss  4.32 | perplexity    75.23\n",
            "| Epoch  31 |   400/ 2983 batches | lr 0.00 | ms/batch 47.05 | train loss  4.35 | perplexity    77.64\n",
            "| Epoch  31 |   500/ 2983 batches | lr 0.00 | ms/batch 47.24 | train loss  4.20 | perplexity    66.92\n",
            "| Epoch  31 |   600/ 2983 batches | lr 0.00 | ms/batch 47.18 | train loss  4.13 | perplexity    61.89\n",
            "| Epoch  31 |   700/ 2983 batches | lr 0.00 | ms/batch 47.14 | train loss  4.22 | perplexity    67.85\n",
            "| Epoch  31 |   800/ 2983 batches | lr 0.00 | ms/batch 47.18 | train loss  4.30 | perplexity    73.90\n",
            "| Epoch  31 |   900/ 2983 batches | lr 0.00 | ms/batch 47.09 | train loss  4.26 | perplexity    71.06\n",
            "| Epoch  31 |  1000/ 2983 batches | lr 0.00 | ms/batch 47.09 | train loss  4.21 | perplexity    67.21\n",
            "| Epoch  31 |  1100/ 2983 batches | lr 0.00 | ms/batch 47.13 | train loss  4.20 | perplexity    66.78\n",
            "| Epoch  31 |  1200/ 2983 batches | lr 0.00 | ms/batch 47.14 | train loss  4.26 | perplexity    71.04\n",
            "| Epoch  31 |  1300/ 2983 batches | lr 0.00 | ms/batch 47.20 | train loss  4.32 | perplexity    75.55\n",
            "| Epoch  31 |  1400/ 2983 batches | lr 0.00 | ms/batch 47.14 | train loss  4.20 | perplexity    66.44\n",
            "| Epoch  31 |  1500/ 2983 batches | lr 0.00 | ms/batch 47.18 | train loss  4.28 | perplexity    72.41\n",
            "| Epoch  31 |  1600/ 2983 batches | lr 0.00 | ms/batch 47.02 | train loss  4.32 | perplexity    75.41\n",
            "| Epoch  31 |  1700/ 2983 batches | lr 0.00 | ms/batch 46.87 | train loss  4.21 | perplexity    67.36\n",
            "| Epoch  31 |  1800/ 2983 batches | lr 0.00 | ms/batch 46.97 | train loss  4.20 | perplexity    66.87\n",
            "| Epoch  31 |  1900/ 2983 batches | lr 0.00 | ms/batch 46.88 | train loss  4.25 | perplexity    69.98\n",
            "| Epoch  31 |  2000/ 2983 batches | lr 0.00 | ms/batch 46.89 | train loss  4.20 | perplexity    66.47\n",
            "| Epoch  31 |  2100/ 2983 batches | lr 0.00 | ms/batch 46.93 | train loss  4.14 | perplexity    62.70\n",
            "| Epoch  31 |  2200/ 2983 batches | lr 0.00 | ms/batch 46.85 | train loss  4.08 | perplexity    59.33\n",
            "| Epoch  31 |  2300/ 2983 batches | lr 0.00 | ms/batch 46.93 | train loss  4.07 | perplexity    58.49\n",
            "| Epoch  31 |  2400/ 2983 batches | lr 0.00 | ms/batch 46.98 | train loss  4.22 | perplexity    67.98\n",
            "| Epoch  31 |  2500/ 2983 batches | lr 0.00 | ms/batch 46.95 | train loss  4.12 | perplexity    61.38\n",
            "| Epoch  31 |  2600/ 2983 batches | lr 0.00 | ms/batch 46.88 | train loss  4.20 | perplexity    66.83\n",
            "| Epoch  31 |  2700/ 2983 batches | lr 0.00 | ms/batch 46.89 | train loss  4.15 | perplexity    63.30\n",
            "| Epoch  31 |  2800/ 2983 batches | lr 0.00 | ms/batch 46.89 | train loss  4.06 | perplexity    58.18\n",
            "| Epoch  31 |  2900/ 2983 batches | lr 0.00 | ms/batch 46.82 | train loss  4.08 | perplexity    58.98\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "| End of epoch  30 | time: 144.62s | valid loss  4.85 | valid perplexity   127.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCNUQphVhtRd",
        "colab_type": "text"
      },
      "source": [
        "## Test a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXynlwkZmPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82a0915b-405b-4274-f9b2-6f697fc7e959"
      },
      "source": [
        "# Load the best saved model.\n",
        "with open(save, 'rb') as file:\n",
        "    model = torch.load(file)\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('| End of evaluation | test loss {:5.2f} | test perplexity {:8.2f}'.format( test_loss, math.exp(test_loss)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| End of evaluation | test loss  4.78 | test perplexity   119.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1U65Qidh1_o",
        "colab_type": "text"
      },
      "source": [
        "## Generate sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB7mdzc2ZmPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_file = 'generated.txt'\n",
        "num_words = 500\n",
        "temp = 1.0\n",
        "display_interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeaBJ89VZmP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d37a4200-00cf-4bd4-b6d4-7192a253c9d4"
      },
      "source": [
        "with open(save, 'rb') as file:\n",
        "  model = torch.load(file).to(device)\n",
        "model.eval()\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "input = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(generated_file, 'w') as file:\n",
        "    with torch.no_grad():\n",
        "      for i in range(num_words):\n",
        "        output, hidden = model(input, hidden)\n",
        "               \n",
        "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "\n",
        "        input.fill_(word_idx)\n",
        "        word = corpus.dictionary.idx2word[word_idx]\n",
        "        file.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "        if i % display_interval == 0:\n",
        "          print('| Generated {}/{} words'.format(i, num_words))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/500 words\n",
            "| Generated 100/500 words\n",
            "| Generated 200/500 words\n",
            "| Generated 300/500 words\n",
            "| Generated 400/500 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuWUnQ8tnVlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}